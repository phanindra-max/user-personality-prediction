{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re    # for regular expressions \n",
    "import nltk  # for text manipulation \n",
    "import string \n",
    "import warnings \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 200) \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP\n",
    "import string, re, nltk\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "#!pip install num2words\n",
    "from num2words import num2words\n",
    "#!pip install pyspellchecker\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import spacy\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Scipy\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Train-test split and cross validation\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "\n",
    "# Others\n",
    "import json\n",
    "import gensim\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data.csv',encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = train.dropna()\n",
    "#test = test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Type</th>\n",
       "      <th>Posts</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 14</th>\n",
       "      <th>Unnamed: 15</th>\n",
       "      <th>Unnamed: 16</th>\n",
       "      <th>Unnamed: 17</th>\n",
       "      <th>Unnamed: 18</th>\n",
       "      <th>Unnamed: 19</th>\n",
       "      <th>Unnamed: 20</th>\n",
       "      <th>Unnamed: 21</th>\n",
       "      <th>Unnamed: 22</th>\n",
       "      <th>Unnamed: 23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "      <td>http://www.youtube.com/watch?v=qsXHcwe3krw|||http://41.media.tumblr.com/tumblr_lfouy03PMA1qa1rooo1_500.jpg|||enfp and intj moments  https://www.youtube.com/watch?v=iz7lE1g4XM4  sportscenter not to...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>'I'm finding the lack of me in these posts very alarming.|||Sex can be boring if it's in the same position often. For example me and my girlfriend are currently in an environment where we have to ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>O</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/watch?v=fHiGbolFFGw|||Of course, to which I say I know; that's my blessing and my curse.|||Does being absolutely positive that you and your best friend c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>O</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the other day.  Esoteric gabbing about the nature of the universe and the idea that every rule and social code being arbitrary constructs created...|||Dear...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>A</td>\n",
       "      <td>'You're fired.|||That's another silly misconception. That approaching is logically is going to be the key to unlocking whatever it is you think you are entitled to.   Nobody wants to be approached...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 Type  \\\n",
       "0           1    O   \n",
       "1           2    A   \n",
       "2           3    O   \n",
       "3           4    O   \n",
       "4           5    A   \n",
       "\n",
       "                                                                                                                                                                                                     Posts  \\\n",
       "0  http://www.youtube.com/watch?v=qsXHcwe3krw|||http://41.media.tumblr.com/tumblr_lfouy03PMA1qa1rooo1_500.jpg|||enfp and intj moments  https://www.youtube.com/watch?v=iz7lE1g4XM4  sportscenter not to...   \n",
       "1  'I'm finding the lack of me in these posts very alarming.|||Sex can be boring if it's in the same position often. For example me and my girlfriend are currently in an environment where we have to ...   \n",
       "2  'Good one  _____   https://www.youtube.com/watch?v=fHiGbolFFGw|||Of course, to which I say I know; that's my blessing and my curse.|||Does being absolutely positive that you and your best friend c...   \n",
       "3  'Dear INTP,   I enjoyed our conversation the other day.  Esoteric gabbing about the nature of the universe and the idea that every rule and social code being arbitrary constructs created...|||Dear...   \n",
       "4  'You're fired.|||That's another silly misconception. That approaching is logically is going to be the key to unlocking whatever it is you think you are entitled to.   Nobody wants to be approached...   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4 Unnamed: 5 Unnamed: 6 Unnamed: 7 Unnamed: 8  \\\n",
       "0        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "1        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "2        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "3        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "4        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "\n",
       "  Unnamed: 9  ... Unnamed: 14 Unnamed: 15 Unnamed: 16 Unnamed: 17 Unnamed: 18  \\\n",
       "0        NaN  ...         NaN         NaN         NaN         NaN         NaN   \n",
       "1        NaN  ...         NaN         NaN         NaN         NaN         NaN   \n",
       "2        NaN  ...         NaN         NaN         NaN         NaN         NaN   \n",
       "3        NaN  ...         NaN         NaN         NaN         NaN         NaN   \n",
       "4        NaN  ...         NaN         NaN         NaN         NaN         NaN   \n",
       "\n",
       "  Unnamed: 19 Unnamed: 20 Unnamed: 21 Unnamed: 22 Unnamed: 23  \n",
       "0         NaN         NaN         NaN         NaN         NaN  \n",
       "1         NaN         NaN         NaN         NaN         NaN  \n",
       "2         NaN         NaN         NaN         NaN         NaN  \n",
       "3         NaN         NaN         NaN         NaN         NaN  \n",
       "4         NaN         NaN         NaN         NaN         NaN  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[['Type','Posts']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[0:2500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2500 entries, 0 to 2499\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Type    2500 non-null   object\n",
      " 1   Posts   2500 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 39.2+ KB\n"
     ]
    }
   ],
   "source": [
    "train_data = train\n",
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Type', ylabel='count'>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='Type',data=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "  \n",
    "# label_encoder object knows how to understand word labels.\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "  \n",
    "# Encode labels in column 'species'.\n",
    "train_data['Type']= label_encoder.fit_transform(train_data['Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = train.drop('Type', axis = 1),  train['Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 40)\n",
    "data_train = pd.concat([X_train, y_train], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation-test split (from test data)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size = 0.5, random_state = 40)\n",
    "data_val, data_test = pd.concat([X_val, y_val], axis = 1), pd.concat([X_test, y_test], axis = 1)\n",
    "\n",
    "# Comparison of sizes of training set, validation set and test set\n",
    "values = np.array([len(data_train), len(data_val), len(data_test)])\n",
    "labels = ['Training set', 'Validation Set', 'Test set']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RegexpTokenizer\n",
    "regexp = RegexpTokenizer(\"[\\w']+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: This is a FUNCTION that CoNvErTs a Text to lowercase\n",
      "Output: this is a function that converts a text to lowercase\n"
     ]
    }
   ],
   "source": [
    "# Converting to lowercase\n",
    "def convert_to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "text = \"This is a FUNCTION that CoNvErTs a Text to lowercase\"\n",
    "print(\"Input: {}\".format(text))\n",
    "print(\"Output: {}\".format(convert_to_lowercase(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  \t This is a string \t \n",
      "Output: This is a string\n"
     ]
    }
   ],
   "source": [
    "# Removing whitespaces\n",
    "def remove_whitespace(text):\n",
    "    return text.strip()\n",
    "\n",
    "text = \" \\t This is a string \\t \"\n",
    "print(\"Input: {}\".format(text))\n",
    "print(\"Output: {}\".format(remove_whitespace(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Here's [an] example? {of} &a string. with.? punctuations!!!!\n",
      "Output: Here's an example of a string with punctuations\n"
     ]
    }
   ],
   "source": [
    "# Removing punctuations\n",
    "def remove_punctuation(text):\n",
    "    punct_str = string.punctuation\n",
    "    punct_str = punct_str.replace(\"'\", \"\") # discarding apostrophe from the string to keep the contractions intact\n",
    "    return text.translate(str.maketrans(\"\", \"\", punct_str))\n",
    "\n",
    "text = \"Here's [an] example? {of} &a string. with.? punctuations!!!!\"\n",
    "print(\"Input: {}\".format(text))\n",
    "print(\"Output: {}\".format(remove_punctuation(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <a href = \"https://www.kaggle.com/datasets/saurabhshahane/ecommerce-text-classification\"> Ecommerce Text Classification </a>\n",
      "Output:  Ecommerce Text Classification \n"
     ]
    }
   ],
   "source": [
    "# Removing HTML tags\n",
    "def remove_html(text):\n",
    "    html = re.compile(r'<.*?>')\n",
    "    return html.sub(r'', text)\n",
    "\n",
    "text = '<a href = \"https://www.kaggle.com/datasets/saurabhshahane/ecommerce-text-classification\"> Ecommerce Text Classification </a>'\n",
    "print(\"Input: {}\".format(text))\n",
    "print(\"Output: {}\".format(remove_html(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: This innovative hd printing technique results in durable and spectacular looking prints ðŸ˜Š\n",
      "Output: This innovative hd printing technique results in durable and spectacular looking prints \n"
     ]
    }
   ],
   "source": [
    "# Removing emojis\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "text = \"This innovative hd printing technique results in durable and spectacular looking prints ðŸ˜Š\"\n",
    "print(\"Input: {}\".format(text))\n",
    "print(\"Output: {}\".format(remove_emoji(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: It's a function that removes links starting with http: or https such as https://en.wikipedia.org/wiki/Unicode_symbols\n",
      "Output: It's a function that removes links starting with http: or https such as \n"
     ]
    }
   ],
   "source": [
    "# Removing other unicode characters\n",
    "def remove_http(text):\n",
    "    http = \"https?://\\S+|www\\.\\S+\" # matching strings beginning with http (but not just \"http\")\n",
    "    pattern = r\"({})\".format(http) # creating pattern\n",
    "    return re.sub(pattern, \"\", text)\n",
    "\n",
    "text = \"It's a function that removes links starting with http: or https such as https://en.wikipedia.org/wiki/Unicode_symbols\"\n",
    "print(\"Input: {}\".format(text))\n",
    "print(\"Output: {}\".format(remove_http(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Original form of the acronym 'fyi' is 'for your information'\n"
     ]
    }
   ],
   "source": [
    "# Dictionary of acronyms\n",
    "acronyms_url = 'https://raw.githubusercontent.com/sugatagh/E-commerce-Text-Classification/main/JSON/english_acronyms.json'\n",
    "acronyms_dict = pd.read_json(acronyms_url, typ = 'series')\n",
    "\n",
    "print(\"Example: Original form of the acronym 'fyi' is '{}'\".format(acronyms_dict['fyi']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acronym</th>\n",
       "      <th>original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aka</td>\n",
       "      <td>also known as</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>asap</td>\n",
       "      <td>as soon as possible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>brb</td>\n",
       "      <td>be right back</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>btw</td>\n",
       "      <td>by the way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dob</td>\n",
       "      <td>date of birth</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  acronym             original\n",
       "0     aka        also known as\n",
       "1    asap  as soon as possible\n",
       "2     brb        be right back\n",
       "3     btw           by the way\n",
       "4     dob        date of birth"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataframe of acronyms\n",
    "pd.DataFrame(acronyms_dict.items(), columns = ['acronym', 'original']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of acronyms\n",
    "acronyms_list = list(acronyms_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: btw you've to fill in the details including dob\n",
      "Output: by the way you've to fill in the details including date of birth\n"
     ]
    }
   ],
   "source": [
    "# Function to convert contractions in a text\n",
    "def convert_acronyms(text):\n",
    "    words = []\n",
    "    for word in regexp.tokenize(text):\n",
    "        if word in acronyms_list:\n",
    "            words = words + acronyms_dict[word].split()\n",
    "        else:\n",
    "            words = words + word.split()\n",
    "    \n",
    "    text_converted = \" \".join(words)\n",
    "    return text_converted\n",
    "\n",
    "text = \"btw you've to fill in the details including dob\"\n",
    "print(\"Input: {}\".format(text))\n",
    "print(\"Output: {}\".format(convert_acronyms(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Original form of the contraction 'aren't' is 'are not'\n"
     ]
    }
   ],
   "source": [
    "# Dictionary of contractions\n",
    "contractions_url = 'https://raw.githubusercontent.com/sugatagh/E-commerce-Text-Classification/main/JSON/english_contractions.json'\n",
    "contractions_dict = pd.read_json(contractions_url, typ = 'series')\n",
    "\n",
    "print(\"Example: Original form of the contraction 'aren't' is '{}'\".format(contractions_dict[\"aren't\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contraction</th>\n",
       "      <th>original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'aight</td>\n",
       "      <td>alright</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ain't</td>\n",
       "      <td>are not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>amn't</td>\n",
       "      <td>am not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arencha</td>\n",
       "      <td>are not you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aren't</td>\n",
       "      <td>are not</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  contraction     original\n",
       "0      'aight      alright\n",
       "1       ain't      are not\n",
       "2       amn't       am not\n",
       "3     arencha  are not you\n",
       "4      aren't      are not"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataframe of contractions\n",
    "pd.DataFrame(contractions_dict.items(), columns = ['contraction', 'original']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPLICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of contractions\n",
    "contractions_list = list(contractions_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: he's doin' fine\n",
      "Output: he is doing fine\n"
     ]
    }
   ],
   "source": [
    "def convert_contractions(text):\n",
    "    words = []\n",
    "    for word in regexp.tokenize(text):\n",
    "        if word in contractions_list:\n",
    "            words = words + contractions_dict[word].split()\n",
    "        else:\n",
    "            words = words + word.split()\n",
    "    \n",
    "    text_converted = \" \".join(words)\n",
    "    return text_converted\n",
    "\n",
    "text = \"he's doin' fine\"\n",
    "print(\"Input: {}\".format(text))\n",
    "print(\"Output: {}\".format(convert_contractions(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'among', 'onto', 'shall', 'thrice', 'thus', 'twice', 'unto', 'us', 'would']\n"
     ]
    }
   ],
   "source": [
    "# Stopwords\n",
    "stops = stopwords.words(\"english\") # stopwords\n",
    "addstops = [\"among\", \"onto\", \"shall\", \"thrice\", \"thus\", \"twice\", \"unto\", \"us\", \"would\"] # additional stopwords\n",
    "allstops = stops + addstops\n",
    "\n",
    "print(allstops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: This is a function that removes stopwords in a given text\n",
      "Output: This function removes stopwords given text\n"
     ]
    }
   ],
   "source": [
    "# Function to remove stopwords from a list of texts\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in regexp.tokenize(text) if word not in allstops])\n",
    "\n",
    "text = \"This is a function that removes stopwords in a given text\"\n",
    "print(\"Input: {}\".format(text))\n",
    "print(\"Output: {}\".format(remove_stopwords(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspellchecker\n",
    "spell = SpellChecker()\n",
    "\n",
    "def pyspellchecker(text):\n",
    "    word_list = regexp.tokenize(text)\n",
    "    word_list_corrected = []\n",
    "    for word in word_list:\n",
    "        if word in spell.unknown(word_list):\n",
    "            word_corrected = spell.correction(word)\n",
    "            if word_corrected == None:\n",
    "                word_list_corrected.append(word)\n",
    "            else:\n",
    "                word_list_corrected.append(word_corrected)\n",
    "        else:\n",
    "            word_list_corrected.append(word)\n",
    "    text_corrected = \" \".join(word_list_corrected)\n",
    "    return text_corrected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Introducing lemmatization as an improvement over stemming\n",
      "Output: introduc lemmat as an improv over stem\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "def text_stemmer(text):\n",
    "    text_stem = \" \".join([stemmer.stem(word) for word in regexp.tokenize(text)])\n",
    "    return text_stem\n",
    "\n",
    "text = \"Introducing lemmatization as an improvement over stemming\"\n",
    "print(\"Input: {}\".format(text))\n",
    "print(\"Output: {}\".format(text_stemmer(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "spacy_lemmatizer = spacy.load(\"en_core_web_sm\", disable = ['parser', 'ner'])\n",
    "#lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def text_lemmatizer(text):\n",
    "    text_spacy = \" \".join([token.lemma_ for token in spacy_lemmatizer(text)])\n",
    "    #text_wordnet = \" \".join([lemmatizer.lemmatize(word) for word in word_tokenize(text)]) # regexp.tokenize(text)\n",
    "    return text_spacy\n",
    "    #return text_wordnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discard_non_alpha(text):\n",
    "    word_list_non_alpha = [word for word in regexp.tokenize(text) if word.isalpha()]\n",
    "    text_non_alpha = \" \".join(word_list_non_alpha)\n",
    "    return text_non_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: He arrived at seven o'clock on Wednesday evening\n",
      "Tokens: ['He', 'arrived', 'at', 'seven', \"o'clock\", 'on', 'Wednesday', 'evening']\n",
      "Tagged Tokens: [('He', 'PRP'), ('arrived', 'VBD'), ('at', 'IN'), ('seven', 'CD'), (\"o'clock\", 'NN'), ('on', 'IN'), ('Wednesday', 'NNP'), ('evening', 'NN')]\n",
      "Output: He arrived o'clock Wednesday evening\n"
     ]
    }
   ],
   "source": [
    "def keep_pos(text):\n",
    "    tokens = regexp.tokenize(text)\n",
    "    tokens_tagged = nltk.pos_tag(tokens)\n",
    "    #keep_tags = ['NN', 'NNS', 'NNP', 'NNPS', 'FW']\n",
    "    keep_tags = ['NN', 'NNS', 'NNP', 'NNPS', 'FW', 'PRP', 'PRPS', 'RB', 'RBR', 'RBS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WPS', 'WRB']\n",
    "    keep_words = [x[0] for x in tokens_tagged if x[1] in keep_tags]\n",
    "    return \" \".join(keep_words)\n",
    "\n",
    "text = \"He arrived at seven o'clock on Wednesday evening\"\n",
    "print(\"Input: {}\".format(text))\n",
    "tokens = regexp.tokenize(text)\n",
    "print(\"Tokens: {}\".format(tokens))\n",
    "tokens_tagged = nltk.pos_tag(tokens)\n",
    "print(\"Tagged Tokens: {}\".format(tokens_tagged))\n",
    "print(\"Output: {}\".format(keep_pos(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabets = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\"]\n",
    "prepositions = [\"about\", \"above\", \"across\", \"after\", \"against\", \"among\", \"around\", \"at\", \"before\", \"behind\", \"below\", \"beside\", \"between\", \"by\", \"down\", \"during\", \"for\", \"from\", \"in\", \"inside\", \"into\", \"near\", \"of\", \"off\", \"on\", \"out\", \"over\", \"through\", \"to\", \"toward\", \"under\", \"up\", \"with\"]\n",
    "prepositions_less_common = [\"aboard\", \"along\", \"amid\", \"as\", \"beneath\", \"beyond\", \"but\", \"concerning\", \"considering\", \"despite\", \"except\", \"following\", \"like\", \"minus\", \"onto\", \"outside\", \"per\", \"plus\", \"regarding\", \"round\", \"since\", \"than\", \"till\", \"underneath\", \"unlike\", \"until\", \"upon\", \"versus\", \"via\", \"within\", \"without\"]\n",
    "coordinating_conjunctions = [\"and\", \"but\", \"for\", \"nor\", \"or\", \"so\", \"and\", \"yet\"]\n",
    "correlative_conjunctions = [\"both\", \"and\", \"either\", \"or\", \"neither\", \"nor\", \"not\", \"only\", \"but\", \"whether\", \"or\"]\n",
    "subordinating_conjunctions = [\"after\", \"although\", \"as\", \"as if\", \"as long as\", \"as much as\", \"as soon as\", \"as though\", \"because\", \"before\", \"by the time\", \"even if\", \"even though\", \"if\", \"in order that\", \"in case\", \"in the event that\", \"lest\", \"now that\", \"once\", \"only\", \"only if\", \"provided that\", \"since\", \"so\", \"supposing\", \"that\", \"than\", \"though\", \"till\", \"unless\", \"until\", \"when\", \"whenever\", \"where\", \"whereas\", \"wherever\", \"whether or not\", \"while\"]\n",
    "others = [\"Ã£\", \"Ã¥\", \"Ã¬\", \"Ã»\", \"Ã»Âªm\", \"Ã»Ã³\", \"Ã»Ã²\", \"Ã¬Ã±\", \"Ã»Âªre\", \"Ã»Âªve\", \"Ã»Âª\", \"Ã»Âªs\", \"Ã»Ã³we\"]\n",
    "additional_stops = alphabets + prepositions + prepositions_less_common + coordinating_conjunctions + correlative_conjunctions + subordinating_conjunctions + others\n",
    "\n",
    "def remove_additional_stopwords(text):\n",
    "    return \" \".join([word for word in regexp.tokenize(text) if word not in additional_stops])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_normalizer(text):\n",
    "    text = convert_to_lowercase(text)\n",
    "    text = remove_whitespace(text)\n",
    "    text = re.sub('\\n' , '', text) # converting text to one line\n",
    "    text = re.sub('\\[.*?\\]', '', text) # removing square brackets\n",
    "    text = remove_http(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_html(text)\n",
    "    text = remove_emoji(text)\n",
    "    text = convert_acronyms(text)\n",
    "    text = convert_contractions(text)\n",
    "    text = remove_stopwords(text)\n",
    "#     text = pyspellchecker(text)\n",
    "    text = text_lemmatizer(text) # text = text_stemmer(text)\n",
    "    text = discard_non_alpha(text)\n",
    "    text = keep_pos(text)\n",
    "    text = remove_additional_stopwords(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>normalized description</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>goodness thank much personalise reply glad picture moreover find comment rather head hi hear draw exercise go much join thank drop photo drawing seriousness love infjs well marvelously inspiring r...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6829</th>\n",
       "      <td>okay firing obsess really sorry long post ahead ignore infp enfp infj enfj maybe infp continue feel use fe fi ti te use ni conclusion stubbornness relate infp likeye path currently kid want make g...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8085</th>\n",
       "      <td>sound version test asperger online doubt feel read expression selectively definitely waste time board enfp secretly need socialization much infp enjoy solitude restlesssorry think op problem intj ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3526</th>\n",
       "      <td>classmate cool sit back row foot chair gossip school try vote entj also look notice year late medrywhat scream seriously try infj infpyou think loud always feel help humanity grow organize thing m...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6638</th>\n",
       "      <td>hmmmz know neighbour sell house blushedjust dancing singe maybe bit front everyone well way think concert fan dancing night come tap shoulder ask please bit less laugh notah room situation let cal...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7839</th>\n",
       "      <td>see fite amount extrovertedperception function description ixtj pretty bias sense istp also take note orangeyou describe aspect oflinke share similarity offer overlappingwell do consider really ev...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3603</th>\n",
       "      <td>say sosx attentionseeking tragically self esteem nt complain picture maybe seem sometimes feel take lot get know relate part head type question recently see question seem ixfp definitely enneagram...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5959</th>\n",
       "      <td>PRON be extremely bitter quite sometimes notice something friend even quite use emojis use emoticon xd xp etc use friendsfamilykinda anyone use try portrayif someone come ask make decision somethi...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5426</th>\n",
       "      <td>honestly point think rather starvation bust ass freelance crap outta town rather chain someone else idea paychequei appreciate mistake year contract position first job ever feel position make syst...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>actualized type intp introvert extroverte sense think feel perceiving judge masculine point feminine point point gender mbti type intp probablyi think health include health part health fitness oh ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6940 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                       normalized description  \\\n",
       "411   goodness thank much personalise reply glad picture moreover find comment rather head hi hear draw exercise go much join thank drop photo drawing seriousness love infjs well marvelously inspiring r...   \n",
       "6829  okay firing obsess really sorry long post ahead ignore infp enfp infj enfj maybe infp continue feel use fe fi ti te use ni conclusion stubbornness relate infp likeye path currently kid want make g...   \n",
       "8085  sound version test asperger online doubt feel read expression selectively definitely waste time board enfp secretly need socialization much infp enjoy solitude restlesssorry think op problem intj ...   \n",
       "3526  classmate cool sit back row foot chair gossip school try vote entj also look notice year late medrywhat scream seriously try infj infpyou think loud always feel help humanity grow organize thing m...   \n",
       "6638  hmmmz know neighbour sell house blushedjust dancing singe maybe bit front everyone well way think concert fan dancing night come tap shoulder ask please bit less laugh notah room situation let cal...   \n",
       "...                                                                                                                                                                                                       ...   \n",
       "7839  see fite amount extrovertedperception function description ixtj pretty bias sense istp also take note orangeyou describe aspect oflinke share similarity offer overlappingwell do consider really ev...   \n",
       "3603  say sosx attentionseeking tragically self esteem nt complain picture maybe seem sometimes feel take lot get know relate part head type question recently see question seem ixfp definitely enneagram...   \n",
       "5959  PRON be extremely bitter quite sometimes notice something friend even quite use emojis use emoticon xd xp etc use friendsfamilykinda anyone use try portrayif someone come ask make decision somethi...   \n",
       "5426  honestly point think rather starvation bust ass freelance crap outta town rather chain someone else idea paychequei appreciate mistake year contract position first job ever feel position make syst...   \n",
       "7608  actualized type intp introvert extroverte sense think feel perceiving judge masculine point feminine point point gender mbti type intp probablyi think health include health part health fitness oh ...   \n",
       "\n",
       "      label  \n",
       "411       4  \n",
       "6829      4  \n",
       "8085      4  \n",
       "3526      4  \n",
       "6638      0  \n",
       "...     ...  \n",
       "7839      1  \n",
       "3603      1  \n",
       "5959      4  \n",
       "5426      2  \n",
       "7608      4  \n",
       "\n",
       "[6940 rows x 2 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Implementing text normalization\n",
    "data_train_norm, data_val_norm, data_test_norm = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "data_train_norm['normalized description'] = data_train['Posts'].apply(text_normalizer)\n",
    "data_val_norm['normalized description'] = data_val['Posts'].apply(text_normalizer)\n",
    "data_test_norm['normalized description'] = data_test['Posts'].apply(text_normalizer)\n",
    "\n",
    "data_train_norm['label'] = data_train['Type']\n",
    "data_val_norm['label'] = data_val['Type']\n",
    "data_test_norm['label'] = data_test['Type']\n",
    "\n",
    "data_train_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and labels\n",
    "X_train_norm, y_train = data_train_norm['normalized description'].tolist(), data_train_norm['label'].tolist()\n",
    "X_val_norm, y_val = data_val_norm['normalized description'].tolist(), data_val_norm['label'].tolist()\n",
    "X_test_norm, y_test = data_test_norm['normalized description'].tolist(), data_test_norm['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "TfidfVec = TfidfVectorizer(ngram_range = (1, 1))\n",
    "X_train_tfidf = TfidfVec.fit_transform(X_train_norm)\n",
    "X_val_tfidf = TfidfVec.transform(X_val_norm)\n",
    "X_test_tfidf = TfidfVec.transform(X_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X_train_tfidf.toarray()\n",
    "x_test = X_test_tfidf.toarray()\n",
    "x_val = X_val_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0  355]\n",
      " [   0    0    0    0  196]\n",
      " [   0    0    0    0   28]\n",
      " [   0    0    0    0   43]\n",
      " [   0    0    0    0 1113]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       355\n",
      "           1       0.00      0.00      0.00       196\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        43\n",
      "           4       0.64      1.00      0.78      1113\n",
      "\n",
      "    accuracy                           0.64      1735\n",
      "   macro avg       0.13      0.20      0.16      1735\n",
      "weighted avg       0.41      0.64      0.50      1735\n",
      "\n",
      "0.6414985590778098\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(gamma='auto')\n",
    "svm.fit(x_train,y_train)\n",
    "y_pred = svm.predict(x_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "svm = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 82   8   1   0  95]\n",
      " [ 15  31   4   0  51]\n",
      " [  1   0   0   0  11]\n",
      " [  6   2   1   0  14]\n",
      " [ 60  23  11   0 452]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.44      0.47       186\n",
      "           1       0.48      0.31      0.38       101\n",
      "           2       0.00      0.00      0.00        12\n",
      "           3       0.00      0.00      0.00        23\n",
      "           4       0.73      0.83      0.77       546\n",
      "\n",
      "    accuracy                           0.65       868\n",
      "   macro avg       0.34      0.32      0.32       868\n",
      "weighted avg       0.62      0.65      0.63       868\n",
      "\n",
      "0.6509216589861752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TruProjects\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\TruProjects\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\TruProjects\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0)\n",
    "gb.fit(x_train,y_train)\n",
    "y_pred = gb.predict(x_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "gba = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(learning_rate=1.0, max_depth=1, random_state=0)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TruProjects\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 74   3   0   0 109]\n",
      " [  9  23   0   0  69]\n",
      " [  0   0   0   0  12]\n",
      " [  6   2   0   0  15]\n",
      " [ 19   2   0   0 525]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.40      0.50       186\n",
      "           1       0.77      0.23      0.35       101\n",
      "           2       0.00      0.00      0.00        12\n",
      "           3       0.00      0.00      0.00        23\n",
      "           4       0.72      0.96      0.82       546\n",
      "\n",
      "    accuracy                           0.72       868\n",
      "   macro avg       0.43      0.32      0.34       868\n",
      "weighted avg       0.69      0.72      0.67       868\n",
      "\n",
      "0.716589861751152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TruProjects\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\TruProjects\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\TruProjects\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_train,y_train)\n",
    "y_pred = lr.predict(x_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "lr = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 92   1   0   0  93]\n",
      " [  9  33   0   0  59]\n",
      " [  1   0   0   0  11]\n",
      " [  5   3   0   4  11]\n",
      " [ 27   7   0   0 512]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.49      0.58       186\n",
      "           1       0.75      0.33      0.46       101\n",
      "           2       0.00      0.00      0.00        12\n",
      "           3       1.00      0.17      0.30        23\n",
      "           4       0.75      0.94      0.83       546\n",
      "\n",
      "    accuracy                           0.74       868\n",
      "   macro avg       0.64      0.39      0.43       868\n",
      "weighted avg       0.73      0.74      0.71       868\n",
      "\n",
      "0.738479262672811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TruProjects\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\TruProjects\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\TruProjects\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(x_train,y_train)\n",
    "y_pred = xgb.predict(x_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "xgb = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[113  10   1   4  58]\n",
      " [ 23  43   2   0  33]\n",
      " [  3   0   0   0   9]\n",
      " [  9   4   0   4   6]\n",
      " [ 83  51  11  13 388]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.61      0.54       186\n",
      "           1       0.40      0.43      0.41       101\n",
      "           2       0.00      0.00      0.00        12\n",
      "           3       0.19      0.17      0.18        23\n",
      "           4       0.79      0.71      0.75       546\n",
      "\n",
      "    accuracy                           0.63       868\n",
      "   macro avg       0.37      0.38      0.38       868\n",
      "weighted avg       0.65      0.63      0.64       868\n",
      "\n",
      "0.631336405529954\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "clf1 = XGBClassifier()\n",
    "clf3 = DecisionTreeClassifier()\n",
    "eclf1 = VotingClassifier(estimators=[('lr', clf1), ('dt', clf3)], voting='hard')\n",
    "eclf1.fit(x_train,y_train)\n",
    "y_pred = eclf1.predict(x_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "vot = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant text normalization processes\n",
    "def convert_to_lowercase(text): return text.lower()\n",
    "\n",
    "contractions_url = 'https://raw.githubusercontent.com/sugatagh/E-commerce-Text-Classification/main/JSON/english_contractions.json'\n",
    "contractions_dict = pd.read_json(contractions_url, typ = 'series')\n",
    "contractions_list = list(contractions_dict.keys())\n",
    "\n",
    "def convert_contractions(text):\n",
    "    words = []\n",
    "    for word in regexp.tokenize(text):\n",
    "        if word in contractions_list:\n",
    "            words = words + contractions_dict[word].split()\n",
    "        else:\n",
    "            words = words + word.split()\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>['oh, my, goodness, thank, you, so, much, for, such, a, lovely, in, depth, and, personalised, reply, i, am, glad, you, like, the, picture, d, moreover, i, found, the, comments, rather, insightful,...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6829</th>\n",
       "      <td>['okay, i, am, firing, since, i, obsess, about, this, more, than, i, really, ought, to, i, am, sorry, long, post, ahead, you, can, ignore, if, you, want, i, am, not, sure, if, i, am, infp, enfp, i...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8085</th>\n",
       "      <td>['i, sound, like, the, male, version, of, you, i, test, positive, for, aspergers, online, but, i, have, my, doubts, i, feel, i, can, read, facial, expressions, and, am, empathetic, selectively, i,...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3526</th>\n",
       "      <td>['dear, classmates, just, a, few, it, is, not, cool, to, sit, in, the, back, rows, just, to, kick, your, feet, up, on, the, chairs, and, gossip, like, you, are, in, high, school, you, are, trying,...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6638</th>\n",
       "      <td>[hmmmz, you, know, my, neighbours, are, selling, their, house, blushed, just, dancing, and, singing, and, maybe, a, tad, bit, jumping, in, front, of, everyone, behind, me, which, was, well, everyb...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7839</th>\n",
       "      <td>['seeing, a, lotta, fi, te, and, not, a, strong, amount, of, extroverted, perception, function, in, your, descriptions, so, either, ixfp, or, ixtj, with, a, pretty, good, bias, toward, ixfp, my, s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3603</th>\n",
       "      <td>[i, would, say, gayle, is, an, infp, 4w5, so, sx, a, very, unhealthy, one, she, is, melodramatic, and, attention, seeking, weird, with, tragically, low, self, esteem, what, nts, who, complain, abo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5959</th>\n",
       "      <td>['i'm, extremely, cold, bitter, and, sarcastic, on, the, outside, on, the, inside, though, i, am, quite, sensitive, i, can, sometimes, notice, when, something, is, up, with, my, entj, friend, even...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5426</th>\n",
       "      <td>['honestly, at, this, point, i, think, i, would, rather, risk, starvation, and, just, bust, my, ass, to, freelance, the, crap, outta, this, town, rather, than, being, chained, to, someone, else's,...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>['actualized, type, intp, who, you, are, introverted, i, 78, 79, extroverted, e, 21, 21, intuitive, n, 65, 63, sensing, s, 34, 38, thinking, t, 68, 97, feeling, f, 31, 03, perceiving, p, 62, 5, ju...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6940 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                       tokens  \\\n",
       "411   ['oh, my, goodness, thank, you, so, much, for, such, a, lovely, in, depth, and, personalised, reply, i, am, glad, you, like, the, picture, d, moreover, i, found, the, comments, rather, insightful,...   \n",
       "6829  ['okay, i, am, firing, since, i, obsess, about, this, more, than, i, really, ought, to, i, am, sorry, long, post, ahead, you, can, ignore, if, you, want, i, am, not, sure, if, i, am, infp, enfp, i...   \n",
       "8085  ['i, sound, like, the, male, version, of, you, i, test, positive, for, aspergers, online, but, i, have, my, doubts, i, feel, i, can, read, facial, expressions, and, am, empathetic, selectively, i,...   \n",
       "3526  ['dear, classmates, just, a, few, it, is, not, cool, to, sit, in, the, back, rows, just, to, kick, your, feet, up, on, the, chairs, and, gossip, like, you, are, in, high, school, you, are, trying,...   \n",
       "6638  [hmmmz, you, know, my, neighbours, are, selling, their, house, blushed, just, dancing, and, singing, and, maybe, a, tad, bit, jumping, in, front, of, everyone, behind, me, which, was, well, everyb...   \n",
       "...                                                                                                                                                                                                       ...   \n",
       "7839  ['seeing, a, lotta, fi, te, and, not, a, strong, amount, of, extroverted, perception, function, in, your, descriptions, so, either, ixfp, or, ixtj, with, a, pretty, good, bias, toward, ixfp, my, s...   \n",
       "3603  [i, would, say, gayle, is, an, infp, 4w5, so, sx, a, very, unhealthy, one, she, is, melodramatic, and, attention, seeking, weird, with, tragically, low, self, esteem, what, nts, who, complain, abo...   \n",
       "5959  ['i'm, extremely, cold, bitter, and, sarcastic, on, the, outside, on, the, inside, though, i, am, quite, sensitive, i, can, sometimes, notice, when, something, is, up, with, my, entj, friend, even...   \n",
       "5426  ['honestly, at, this, point, i, think, i, would, rather, risk, starvation, and, just, bust, my, ass, to, freelance, the, crap, outta, this, town, rather, than, being, chained, to, someone, else's,...   \n",
       "7608  ['actualized, type, intp, who, you, are, introverted, i, 78, 79, extroverted, e, 21, 21, intuitive, n, 65, 63, sensing, s, 34, 38, thinking, t, 68, 97, feeling, f, 31, 03, perceiving, p, 62, 5, ju...   \n",
       "\n",
       "      Type  \n",
       "411      4  \n",
       "6829     4  \n",
       "8085     4  \n",
       "3526     4  \n",
       "6638     0  \n",
       "...    ...  \n",
       "7839     1  \n",
       "3603     1  \n",
       "5959     4  \n",
       "5426     2  \n",
       "7608     4  \n",
       "\n",
       "[6940 rows x 2 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for df in [data_train, data_val, data_test]:\n",
    "    df['tokens'] = (df[\"Posts\"].apply(convert_to_lowercase)\n",
    "                                     .apply(convert_contractions)\n",
    "                                     .apply(regexp.tokenize))\n",
    "data_train[['tokens', 'Type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the pre-trained Word2Vec model\n",
    "word2vec_path = 'GoogleNews-vectors-negative300.bin'\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some useful functions for Word2Vec\n",
    "def get_average_word2vec(tokens_list, vector, generate_missing = False, k = 300):\n",
    "    if len(tokens_list) < 1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis = 0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "def get_word2vec_embeddings(vectors, tokens, generate_missing = False):\n",
    "    embeddings = tokens.apply(lambda x: get_average_word2vec(x, vectors, generate_missing = generate_missing))\n",
    "    return list(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec embedding\n",
    "X_train_embed = get_word2vec_embeddings(word2vec, data_train['tokens'])\n",
    "X_val_embed = get_word2vec_embeddings(word2vec, data_val['tokens'])\n",
    "X_test_embed = get_word2vec_embeddings(word2vec, data_test['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to Compressed Sparse Row matrix\n",
    "X_train_w2v = scipy.sparse.csr_matrix(X_train_embed)\n",
    "X_val_w2v = scipy.sparse.csr_matrix(X_val_embed)\n",
    "X_test_w2v = scipy.sparse.csr_matrix(X_test_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X_train_w2v.toarray()\n",
    "x_test = X_test_w2v.toarray()\n",
    "x_val = X_val_w2v.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0 186]\n",
      " [  0   0   0   0 101]\n",
      " [  0   0   0   0  12]\n",
      " [  0   0   0   0  23]\n",
      " [  0   0   0   0 546]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       186\n",
      "           1       0.00      0.00      0.00       101\n",
      "           2       0.00      0.00      0.00        12\n",
      "           3       0.00      0.00      0.00        23\n",
      "           4       0.63      1.00      0.77       546\n",
      "\n",
      "    accuracy                           0.63       868\n",
      "   macro avg       0.13      0.20      0.15       868\n",
      "weighted avg       0.40      0.63      0.49       868\n",
      "\n",
      "0.6290322580645161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TruProjects\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\TruProjects\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\TruProjects\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(gamma='auto')\n",
    "svm.fit(x_train,y_train)\n",
    "y_pred = svm.predict(x_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "svm1 = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 48   6   0   1 131]\n",
      " [ 12  15   0   0  74]\n",
      " [  2   1   0   0   9]\n",
      " [  6   4   0   0  13]\n",
      " [ 61  16   0   4 465]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.26      0.30       186\n",
      "           1       0.36      0.15      0.21       101\n",
      "           2       0.00      0.00      0.00        12\n",
      "           3       0.00      0.00      0.00        23\n",
      "           4       0.67      0.85      0.75       546\n",
      "\n",
      "    accuracy                           0.61       868\n",
      "   macro avg       0.28      0.25      0.25       868\n",
      "weighted avg       0.54      0.61      0.56       868\n",
      "\n",
      "0.6082949308755761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TruProjects\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\TruProjects\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\TruProjects\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0)\n",
    "gb.fit(x_train,y_train)\n",
    "y_pred = gb.predict(x_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "gb1 = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0 186]\n",
      " [  0   0   0   0 101]\n",
      " [  0   0   0   0  12]\n",
      " [  0   0   0   0  23]\n",
      " [  0   0   0   0 546]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       186\n",
      "           1       0.00      0.00      0.00       101\n",
      "           2       0.00      0.00      0.00        12\n",
      "           3       0.00      0.00      0.00        23\n",
      "           4       0.63      1.00      0.77       546\n",
      "\n",
      "    accuracy                           0.63       868\n",
      "   macro avg       0.13      0.20      0.15       868\n",
      "weighted avg       0.40      0.63      0.49       868\n",
      "\n",
      "0.6290322580645161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TruProjects\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "C:\\Users\\TruProjects\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\TruProjects\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\TruProjects\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_train,y_train)\n",
    "y_pred = lr.predict(x_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "lr1 = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 26   2   0   0 158]\n",
      " [  7   6   0   0  88]\n",
      " [  0   0   0   0  12]\n",
      " [  3   0   0   0  20]\n",
      " [ 37   2   0   0 507]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.14      0.20       186\n",
      "           1       0.60      0.06      0.11       101\n",
      "           2       0.00      0.00      0.00        12\n",
      "           3       0.00      0.00      0.00        23\n",
      "           4       0.65      0.93      0.76       546\n",
      "\n",
      "    accuracy                           0.62       868\n",
      "   macro avg       0.32      0.23      0.21       868\n",
      "weighted avg       0.55      0.62      0.53       868\n",
      "\n",
      "0.6209677419354839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TruProjects\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\TruProjects\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\TruProjects\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(x_train,y_train)\n",
    "y_pred = xgb.predict(x_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "xgb1 = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 65  22   6  10  83]\n",
      " [ 32  19   1   0  49]\n",
      " [  4   1   0   0   7]\n",
      " [  8   0   1   1  13]\n",
      " [133  60  17  13 323]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.35      0.30       186\n",
      "           1       0.19      0.19      0.19       101\n",
      "           2       0.00      0.00      0.00        12\n",
      "           3       0.04      0.04      0.04        23\n",
      "           4       0.68      0.59      0.63       546\n",
      "\n",
      "    accuracy                           0.47       868\n",
      "   macro avg       0.24      0.23      0.23       868\n",
      "weighted avg       0.51      0.47      0.49       868\n",
      "\n",
      "0.4700460829493088\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "clf1 = XGBClassifier()\n",
    "clf3 = DecisionTreeClassifier()\n",
    "eclf1 = VotingClassifier(estimators=[('lr', clf1), ('dt', clf3)], voting='hard')\n",
    "eclf1.fit(x_train,y_train)\n",
    "y_pred = eclf1.predict(x_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "vot1 = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import matplotlib.patches as patches\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import re,string,unicodedata\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from string import punctuation\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from scipy import interp\n",
    "\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "import liwc\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer, HashingVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import itertools\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwcPath = r'LIWC2015_English.dic'\n",
    "parse, category_names = liwc.load_token_parser(liwcPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Type</th>\n",
       "      <th>Posts</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 14</th>\n",
       "      <th>Unnamed: 15</th>\n",
       "      <th>Unnamed: 16</th>\n",
       "      <th>Unnamed: 17</th>\n",
       "      <th>Unnamed: 18</th>\n",
       "      <th>Unnamed: 19</th>\n",
       "      <th>Unnamed: 20</th>\n",
       "      <th>Unnamed: 21</th>\n",
       "      <th>Unnamed: 22</th>\n",
       "      <th>Unnamed: 23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "      <td>http://www.youtube.com/watch?v=qsXHcwe3krw|||http://41.media.tumblr.com/tumblr_lfouy03PMA1qa1rooo1_500.jpg|||enfp and intj moments  https://www.youtube.com/watch?v=iz7lE1g4XM4  sportscenter not to...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>'I'm finding the lack of me in these posts very alarming.|||Sex can be boring if it's in the same position often. For example me and my girlfriend are currently in an environment where we have to ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>O</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/watch?v=fHiGbolFFGw|||Of course, to which I say I know; that's my blessing and my curse.|||Does being absolutely positive that you and your best friend c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>O</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the other day.  Esoteric gabbing about the nature of the universe and the idea that every rule and social code being arbitrary constructs created...|||Dear...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>A</td>\n",
       "      <td>'You're fired.|||That's another silly misconception. That approaching is logically is going to be the key to unlocking whatever it is you think you are entitled to.   Nobody wants to be approached...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 Type  \\\n",
       "0           1    O   \n",
       "1           2    A   \n",
       "2           3    O   \n",
       "3           4    O   \n",
       "4           5    A   \n",
       "\n",
       "                                                                                                                                                                                                     Posts  \\\n",
       "0  http://www.youtube.com/watch?v=qsXHcwe3krw|||http://41.media.tumblr.com/tumblr_lfouy03PMA1qa1rooo1_500.jpg|||enfp and intj moments  https://www.youtube.com/watch?v=iz7lE1g4XM4  sportscenter not to...   \n",
       "1  'I'm finding the lack of me in these posts very alarming.|||Sex can be boring if it's in the same position often. For example me and my girlfriend are currently in an environment where we have to ...   \n",
       "2  'Good one  _____   https://www.youtube.com/watch?v=fHiGbolFFGw|||Of course, to which I say I know; that's my blessing and my curse.|||Does being absolutely positive that you and your best friend c...   \n",
       "3  'Dear INTP,   I enjoyed our conversation the other day.  Esoteric gabbing about the nature of the universe and the idea that every rule and social code being arbitrary constructs created...|||Dear...   \n",
       "4  'You're fired.|||That's another silly misconception. That approaching is logically is going to be the key to unlocking whatever it is you think you are entitled to.   Nobody wants to be approached...   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4 Unnamed: 5 Unnamed: 6 Unnamed: 7 Unnamed: 8  \\\n",
       "0        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "1        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "2        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "3        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "4        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "\n",
       "  Unnamed: 9  ... Unnamed: 14 Unnamed: 15 Unnamed: 16 Unnamed: 17 Unnamed: 18  \\\n",
       "0        NaN  ...         NaN         NaN         NaN         NaN         NaN   \n",
       "1        NaN  ...         NaN         NaN         NaN         NaN         NaN   \n",
       "2        NaN  ...         NaN         NaN         NaN         NaN         NaN   \n",
       "3        NaN  ...         NaN         NaN         NaN         NaN         NaN   \n",
       "4        NaN  ...         NaN         NaN         NaN         NaN         NaN   \n",
       "\n",
       "  Unnamed: 19 Unnamed: 20 Unnamed: 21 Unnamed: 22 Unnamed: 23  \n",
       "0         NaN         NaN         NaN         NaN         NaN  \n",
       "1         NaN         NaN         NaN         NaN         NaN  \n",
       "2         NaN         NaN         NaN         NaN         NaN  \n",
       "3         NaN         NaN         NaN         NaN         NaN  \n",
       "4         NaN         NaN         NaN         NaN         NaN  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading the dataframe\n",
    "df = pd.read_csv('data.csv',encoding='ISO-8859-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Type','Posts']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordopt(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub(\"\\\\W\",\" \",text) \n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Posts'] = df.Posts.apply(lambda x : wordopt(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_stopwords = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_eng_stopwords(text):\n",
    "    token_text = nltk.word_tokenize(text)\n",
    "    remove_stop = [word for word in token_text if word not in eng_stopwords]\n",
    "    join_text = ' '.join(remove_stop)\n",
    "    return join_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Posts'] = df.Posts.apply(lambda x : remove_eng_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0, 1, 3, 2])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import label encoder\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# label_encoder object knows how to understand word labels.\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "# Encode labels in column 'species'.\n",
    "df['Type']= label_encoder.fit_transform(df['Type'])\n",
    "\n",
    "df['Type'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "lemm = WordNetLemmatizer()\n",
    "def word_lemmatizer(text):\n",
    "    token_text = nltk.word_tokenize(text)\n",
    "    remove_stop = [lemm.lemmatize(w) for w in token_text]\n",
    "    join_text = ' '.join(remove_stop)\n",
    "    return join_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Posts'] =df.Posts.apply(lambda x : word_lemmatizer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word_STOPWORDS = [\"e\", \"te\", \"i\", \"me\", \"qe\", \"ne\", \"nje\", \"a\", \"per\", \"sh\", \"nga\", \"ka\", \"u\", \"eshte\", \"dhe\", \"shih\", \"nuk\",\n",
    "             \"m\", \"dicka\", \"ose\", \"si\", \"shume\", \"etj\", \"se\", \"pa\", \"sipas\", \"s\", \"t\", \"dikujt\", \"dike\", \"mire\", \"vet\",\n",
    "             \"bej\", \"ai\", \"vend\", \"prej\", \"ja\", \"duke\", \"tjeter\", \"kur\", \"ia\", \"ku\", \"ta\", \"keq\", \"dy\", \"ben\", \"bere\",\n",
    "             \"behet\", \"dickaje\", \"edhe\", \"madhe\", \"la\", \"sa\", \"gjate\", \"zakonisht\", \"pas\", \"veta\", \"mbi\", \"disa\", \"iu\",\n",
    "             \"mos\", \"c\", \"para\", \"dikush\", \"gje\", \"be\", \"pak\", \"tek\", \"fare\", \"beri\", \"po\", \"bie\", \"k\", \"do\", \"gjithe\",\n",
    "             \"vete\", \"mund\", \"kam\", \"le\", \"jo\", \"beje\", \"tij\", \"kane\", \"ishte\", \"jane\", \"vjen\", \"ate\", \"kete\", \"neper\",\n",
    "             \"cdo\", \"na\", \"marre\", \"merr\", \"mori\", \"rri\", \"deri\", \"b\", \"kishte\", \"mban\", \"perpara\", \"tyre\", \"marr\",\n",
    "             \"gjitha\", \"as\", \"vetem\", \"nen\", \"here\", \"tjera\", \"tjeret\", \"drejt\", \"qenet\", \"ndonje\", \"nese\", \"jap\",\n",
    "             \"merret\", \"rreth\", \"lloj\", \"dot\", \"saj\", \"nder\", \"ndersa\", \"cila\", \"veten\", \"ma\", \"ndaj\", \"mes\", \"ajo\",\n",
    "             \"cilen\", \"por\", \"ndermjet\", \"prapa\", \"mi\", \"tere\", \"jam\", \"ashtu\", \"kesaj\", \"tille\", \"behem\", \"cilat\",\n",
    "             \"kjo\", \"menjehere\", \"ca\", \"je\", \"aq\", \"aty\", \"prane\", \"ato\", \"pasur\", \"qene\", \"cilin\", \"teper\", \"njera\",\n",
    "             \"tej\", \"krejt\", \"kush\", \"bejne\", \"ti\", \"bene\", \"midis\", \"cili\", \"ende\", \"keto\", \"kemi\", \"sic\", \"kryer\",\n",
    "             \"cilit\", \"atij\", \"gjithnje\", \"andej\", \"siper\", \"sikur\", \"ketej\", \"ciles\", \"ky\", \"papritur\", \"ua\",\n",
    "             \"kryesisht\", \"gjithcka\", \"pasi\", \"kryhet\", \"mjaft\", \"ketij\", \"perbashket\", \"ata\", \"atje\", \"vazhdimisht\",\n",
    "             \"kurre\", \"tone\", \"keshtu\", \"une\", \"sapo\", \"rralle\", \"vetes\", \"ishin\", \"afert\", \"tjetren\", \"ketu\", \"cfare\",\n",
    "             \"to\", \"anes\", \"jemi\", \"asaj\", \"secila\", \"kundrejt\", \"ketyre\", \"pse\", \"tilla\", \"mua\", \"nepermjet\", \"cilet\",\n",
    "             \"ndryshe\", \"kishin\", \"ju\", \"tani\", \"atyre\", \"dic\", \"yne\", \"kudo\", \"sone\", \"sepse\", \"cilave\", \"kem\", \"ty\",\n",
    "             \"t'i\", \"nbsp\", \"tha\", \"re\", \"the\",\"jr\",\"t\"]\n",
    "stop = set(stopwords.words('english'))\n",
    "punctuation = list(string.punctuation)\n",
    "stop.update(punctuation)\n",
    "text_unknows= Word_STOPWORDS\n",
    "stop.update(text_unknows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "#Removing the square brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "# Removing URL's\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "#Removing the stopwords from text\n",
    "def remove_stopwords(text):\n",
    "    final_text = []\n",
    "    for i in text.split():\n",
    "        if i.strip().lower() not in stop:\n",
    "            final_text.append(i.strip())\n",
    "    return \" \".join(final_text)\n",
    "#Removing the noisy text\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Posts'] = df.Posts.apply(lambda x : denoise_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Posts']=df['Posts'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def punctuation_removal(text):\n",
    "    all_list = [char for char in text if char not in string.punctuation]\n",
    "    clean_str = ''.join(all_list)\n",
    "    return clean_str\n",
    "df['Posts'] = df['Posts'].apply(punctuation_removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "words = []\n",
    "\n",
    "for i in range(0,len(df)):\n",
    "    review = re.sub('[^a-zA-Z0-9]',' ',df['Posts'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split() \n",
    "    review = list(category for token in review for category in parse(token))\n",
    "    statements = ' '.join(review)\n",
    "    corpus.append(statements)\n",
    "    words.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer=TfidfVectorizer(max_features=5000)\n",
    "X_fit=vectorizer.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed=X_fit.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>achieve</th>\n",
       "      <th>achievement</th>\n",
       "      <th>adj</th>\n",
       "      <th>adjectives</th>\n",
       "      <th>adverb</th>\n",
       "      <th>adverbs</th>\n",
       "      <th>affect</th>\n",
       "      <th>affiliation</th>\n",
       "      <th>anger</th>\n",
       "      <th>anx</th>\n",
       "      <th>...</th>\n",
       "      <th>tentat</th>\n",
       "      <th>tentative</th>\n",
       "      <th>they</th>\n",
       "      <th>time</th>\n",
       "      <th>verb</th>\n",
       "      <th>verbs</th>\n",
       "      <th>we</th>\n",
       "      <th>words</th>\n",
       "      <th>work</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.043255</td>\n",
       "      <td>0.043255</td>\n",
       "      <td>0.082724</td>\n",
       "      <td>0.082724</td>\n",
       "      <td>0.035988</td>\n",
       "      <td>0.035988</td>\n",
       "      <td>0.230109</td>\n",
       "      <td>0.122485</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022479</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039582</td>\n",
       "      <td>0.039582</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.201392</td>\n",
       "      <td>0.161777</td>\n",
       "      <td>0.183347</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125855</td>\n",
       "      <td>0.079264</td>\n",
       "      <td>0.022047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.038478</td>\n",
       "      <td>0.038478</td>\n",
       "      <td>0.100251</td>\n",
       "      <td>0.100251</td>\n",
       "      <td>0.059758</td>\n",
       "      <td>0.059758</td>\n",
       "      <td>0.319838</td>\n",
       "      <td>0.106822</td>\n",
       "      <td>0.038875</td>\n",
       "      <td>0.013331</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032010</td>\n",
       "      <td>0.032010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.149292</td>\n",
       "      <td>0.208937</td>\n",
       "      <td>0.217465</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.132215</td>\n",
       "      <td>0.085467</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.041188</td>\n",
       "      <td>0.041188</td>\n",
       "      <td>0.082195</td>\n",
       "      <td>0.082195</td>\n",
       "      <td>0.074532</td>\n",
       "      <td>0.074532</td>\n",
       "      <td>0.338938</td>\n",
       "      <td>0.066892</td>\n",
       "      <td>0.026008</td>\n",
       "      <td>0.016054</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048826</td>\n",
       "      <td>0.048826</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.220874</td>\n",
       "      <td>0.184854</td>\n",
       "      <td>0.202826</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.138673</td>\n",
       "      <td>0.066899</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.022771</td>\n",
       "      <td>0.022771</td>\n",
       "      <td>0.092950</td>\n",
       "      <td>0.092950</td>\n",
       "      <td>0.090937</td>\n",
       "      <td>0.090937</td>\n",
       "      <td>0.239521</td>\n",
       "      <td>0.062067</td>\n",
       "      <td>0.033463</td>\n",
       "      <td>0.008606</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070261</td>\n",
       "      <td>0.070261</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.082613</td>\n",
       "      <td>0.192008</td>\n",
       "      <td>0.210589</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.167271</td>\n",
       "      <td>0.078627</td>\n",
       "      <td>0.012662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.047817</td>\n",
       "      <td>0.047817</td>\n",
       "      <td>0.119281</td>\n",
       "      <td>0.119281</td>\n",
       "      <td>0.040579</td>\n",
       "      <td>0.040579</td>\n",
       "      <td>0.333871</td>\n",
       "      <td>0.057347</td>\n",
       "      <td>0.062803</td>\n",
       "      <td>0.004970</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038188</td>\n",
       "      <td>0.038188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.104955</td>\n",
       "      <td>0.190762</td>\n",
       "      <td>0.209838</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.121639</td>\n",
       "      <td>0.100369</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8670</th>\n",
       "      <td>0.037392</td>\n",
       "      <td>0.037392</td>\n",
       "      <td>0.087946</td>\n",
       "      <td>0.087946</td>\n",
       "      <td>0.055998</td>\n",
       "      <td>0.055998</td>\n",
       "      <td>0.293053</td>\n",
       "      <td>0.026693</td>\n",
       "      <td>0.080953</td>\n",
       "      <td>0.016656</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079988</td>\n",
       "      <td>0.079988</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.149225</td>\n",
       "      <td>0.205113</td>\n",
       "      <td>0.231751</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.130557</td>\n",
       "      <td>0.085429</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8671</th>\n",
       "      <td>0.018921</td>\n",
       "      <td>0.018921</td>\n",
       "      <td>0.096112</td>\n",
       "      <td>0.096112</td>\n",
       "      <td>0.053236</td>\n",
       "      <td>0.053236</td>\n",
       "      <td>0.346570</td>\n",
       "      <td>0.079077</td>\n",
       "      <td>0.010427</td>\n",
       "      <td>0.003576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053229</td>\n",
       "      <td>0.053229</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.106398</td>\n",
       "      <td>0.236739</td>\n",
       "      <td>0.255609</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.126976</td>\n",
       "      <td>0.120348</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8672</th>\n",
       "      <td>0.047948</td>\n",
       "      <td>0.047948</td>\n",
       "      <td>0.071764</td>\n",
       "      <td>0.071764</td>\n",
       "      <td>0.038296</td>\n",
       "      <td>0.038296</td>\n",
       "      <td>0.301306</td>\n",
       "      <td>0.057504</td>\n",
       "      <td>0.043598</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028719</td>\n",
       "      <td>0.028719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.153080</td>\n",
       "      <td>0.193675</td>\n",
       "      <td>0.210412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.124363</td>\n",
       "      <td>0.076681</td>\n",
       "      <td>0.014664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8673</th>\n",
       "      <td>0.023307</td>\n",
       "      <td>0.023307</td>\n",
       "      <td>0.068477</td>\n",
       "      <td>0.068477</td>\n",
       "      <td>0.076274</td>\n",
       "      <td>0.076274</td>\n",
       "      <td>0.271232</td>\n",
       "      <td>0.051765</td>\n",
       "      <td>0.015699</td>\n",
       "      <td>0.013459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062046</td>\n",
       "      <td>0.062046</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.139523</td>\n",
       "      <td>0.228583</td>\n",
       "      <td>0.250538</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.153716</td>\n",
       "      <td>0.108718</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8674</th>\n",
       "      <td>0.031177</td>\n",
       "      <td>0.031177</td>\n",
       "      <td>0.059254</td>\n",
       "      <td>0.059254</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.228049</td>\n",
       "      <td>0.035610</td>\n",
       "      <td>0.017999</td>\n",
       "      <td>0.009258</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063727</td>\n",
       "      <td>0.063727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.165892</td>\n",
       "      <td>0.251713</td>\n",
       "      <td>0.281327</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.155506</td>\n",
       "      <td>0.053421</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8675 rows Ã— 110 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       achieve  achievement       adj  adjectives    adverb   adverbs  \\\n",
       "0     0.043255     0.043255  0.082724    0.082724  0.035988  0.035988   \n",
       "1     0.038478     0.038478  0.100251    0.100251  0.059758  0.059758   \n",
       "2     0.041188     0.041188  0.082195    0.082195  0.074532  0.074532   \n",
       "3     0.022771     0.022771  0.092950    0.092950  0.090937  0.090937   \n",
       "4     0.047817     0.047817  0.119281    0.119281  0.040579  0.040579   \n",
       "...        ...          ...       ...         ...       ...       ...   \n",
       "8670  0.037392     0.037392  0.087946    0.087946  0.055998  0.055998   \n",
       "8671  0.018921     0.018921  0.096112    0.096112  0.053236  0.053236   \n",
       "8672  0.047948     0.047948  0.071764    0.071764  0.038296  0.038296   \n",
       "8673  0.023307     0.023307  0.068477    0.068477  0.076274  0.076274   \n",
       "8674  0.031177     0.031177  0.059254    0.059254  0.065217  0.065217   \n",
       "\n",
       "        affect  affiliation     anger       anx  ...    tentat  tentative  \\\n",
       "0     0.230109     0.122485  0.000000  0.022479  ...  0.039582   0.039582   \n",
       "1     0.319838     0.106822  0.038875  0.013331  ...  0.032010   0.032010   \n",
       "2     0.338938     0.066892  0.026008  0.016054  ...  0.048826   0.048826   \n",
       "3     0.239521     0.062067  0.033463  0.008606  ...  0.070261   0.070261   \n",
       "4     0.333871     0.057347  0.062803  0.004970  ...  0.038188   0.038188   \n",
       "...        ...          ...       ...       ...  ...       ...        ...   \n",
       "8670  0.293053     0.026693  0.080953  0.016656  ...  0.079988   0.079988   \n",
       "8671  0.346570     0.079077  0.010427  0.003576  ...  0.053229   0.053229   \n",
       "8672  0.301306     0.057504  0.043598  0.000000  ...  0.028719   0.028719   \n",
       "8673  0.271232     0.051765  0.015699  0.013459  ...  0.062046   0.062046   \n",
       "8674  0.228049     0.035610  0.017999  0.009258  ...  0.063727   0.063727   \n",
       "\n",
       "      they      time      verb     verbs   we     words      work       you  \n",
       "0      0.0  0.201392  0.161777  0.183347  0.0  0.125855  0.079264  0.022047  \n",
       "1      0.0  0.149292  0.208937  0.217465  0.0  0.132215  0.085467  0.000000  \n",
       "2      0.0  0.220874  0.184854  0.202826  0.0  0.138673  0.066899  0.000000  \n",
       "3      0.0  0.082613  0.192008  0.210589  0.0  0.167271  0.078627  0.012662  \n",
       "4      0.0  0.104955  0.190762  0.209838  0.0  0.121639  0.100369  0.000000  \n",
       "...    ...       ...       ...       ...  ...       ...       ...       ...  \n",
       "8670   0.0  0.149225  0.205113  0.231751  0.0  0.130557  0.085429  0.000000  \n",
       "8671   0.0  0.106398  0.236739  0.255609  0.0  0.126976  0.120348  0.000000  \n",
       "8672   0.0  0.153080  0.193675  0.210412  0.0  0.124363  0.076681  0.014664  \n",
       "8673   0.0  0.139523  0.228583  0.250538  0.0  0.153716  0.108718  0.000000  \n",
       "8674   0.0  0.165892  0.251713  0.281327  0.0  0.155506  0.053421  0.000000  \n",
       "\n",
       "[8675 rows x 110 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = vectorizer.get_feature_names()\n",
    "df_count = pd.DataFrame(X_transformed.toarray(),columns = features)\n",
    "df_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df['Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({4: 5563, 0: 1774, 1: 979, 3: 217, 2: 142})\n"
     ]
    }
   ],
   "source": [
    "counter = Counter(y)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = SMOTE(random_state = 101)\n",
    "x, y = oversample.fit_resample(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1684    2 2824  994   59]\n",
      " [ 961   54 3730  770   48]\n",
      " [ 925    0 4112  483   43]\n",
      " [1506   17 2411 1568   61]\n",
      " [1008   37 3685  724  109]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.30      0.29      5563\n",
      "           1       0.49      0.01      0.02      5563\n",
      "           2       0.25      0.74      0.37      5563\n",
      "           3       0.35      0.28      0.31      5563\n",
      "           4       0.34      0.02      0.04      5563\n",
      "\n",
      "    accuracy                           0.27     27815\n",
      "   macro avg       0.34      0.27      0.20     27815\n",
      "weighted avg       0.34      0.27      0.20     27815\n",
      "\n",
      "0.2706093834262089\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(gamma='auto')\n",
    "svm.fit(x,y)\n",
    "y_pred = svm.predict(x)\n",
    "print(confusion_matrix(y,y_pred))\n",
    "print(classification_report(y,y_pred))\n",
    "print(accuracy_score(y, y_pred))\n",
    "svm3 = accuracy_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2338  773  655  757 1040]\n",
      " [ 786 2807  610  555  805]\n",
      " [ 348  402 4358  203  252]\n",
      " [ 488  494  396 3901  284]\n",
      " [ 612  428  313  315 3895]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.42      0.46      5563\n",
      "           1       0.57      0.50      0.54      5563\n",
      "           2       0.69      0.78      0.73      5563\n",
      "           3       0.68      0.70      0.69      5563\n",
      "           4       0.62      0.70      0.66      5563\n",
      "\n",
      "    accuracy                           0.62     27815\n",
      "   macro avg       0.61      0.62      0.62     27815\n",
      "weighted avg       0.61      0.62      0.62     27815\n",
      "\n",
      "0.6219306129786086\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0)\n",
    "gb.fit(x,y)\n",
    "y_pred = gb.predict(x)\n",
    "print(confusion_matrix(y,y_pred))\n",
    "print(classification_report(y,y_pred))\n",
    "print(accuracy_score(y, y_pred))\n",
    "gb3 = accuracy_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1838  771  730 1300  924]\n",
      " [ 754 2028  813 1012  956]\n",
      " [ 775  735 2309  663 1081]\n",
      " [1143  580  612 2551  677]\n",
      " [ 901  904 1070  872 1816]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.33      0.33      5563\n",
      "           1       0.40      0.36      0.38      5563\n",
      "           2       0.42      0.42      0.42      5563\n",
      "           3       0.40      0.46      0.43      5563\n",
      "           4       0.33      0.33      0.33      5563\n",
      "\n",
      "    accuracy                           0.38     27815\n",
      "   macro avg       0.38      0.38      0.38     27815\n",
      "weighted avg       0.38      0.38      0.38     27815\n",
      "\n",
      "0.37900413445982384\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x,y)\n",
    "y_pred = lr.predict(x)\n",
    "print(confusion_matrix(y,y_pred))\n",
    "print(classification_report(y,y_pred))\n",
    "print(accuracy_score(y, y_pred))\n",
    "lr3 = accuracy_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5422    4    0    1  136]\n",
      " [   4 5514    0    0   45]\n",
      " [   0    0 5563    0    0]\n",
      " [   0    0    0 5563    0]\n",
      " [  26    4    0    0 5533]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98      5563\n",
      "           1       1.00      0.99      0.99      5563\n",
      "           2       1.00      1.00      1.00      5563\n",
      "           3       1.00      1.00      1.00      5563\n",
      "           4       0.97      0.99      0.98      5563\n",
      "\n",
      "    accuracy                           0.99     27815\n",
      "   macro avg       0.99      0.99      0.99     27815\n",
      "weighted avg       0.99      0.99      0.99     27815\n",
      "\n",
      "0.9920905985978788\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(x,y)\n",
    "y_pred = xgb.predict(x)\n",
    "print(confusion_matrix(y,y_pred))\n",
    "print(classification_report(y,y_pred))\n",
    "print(accuracy_score(y, y_pred))\n",
    "xgb3 = accuracy_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5563    0    0    0    0]\n",
      " [   4 5559    0    0    0]\n",
      " [   0    0 5563    0    0]\n",
      " [   0    0    0 5563    0]\n",
      " [  26    4    0    0 5533]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      5563\n",
      "           1       1.00      1.00      1.00      5563\n",
      "           2       1.00      1.00      1.00      5563\n",
      "           3       1.00      1.00      1.00      5563\n",
      "           4       1.00      0.99      1.00      5563\n",
      "\n",
      "    accuracy                           1.00     27815\n",
      "   macro avg       1.00      1.00      1.00     27815\n",
      "weighted avg       1.00      1.00      1.00     27815\n",
      "\n",
      "0.9987776379651268\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "clf1 = XGBClassifier()\n",
    "clf3 = DecisionTreeClassifier()\n",
    "eclf1 = VotingClassifier(estimators=[('lr', clf1), ('dt', clf3)], voting='hard')\n",
    "eclf1.fit(x, y)\n",
    "y_pred = eclf1.predict(x_test)\n",
    "print(confusion_matrix(y,y_pred))\n",
    "print(classification_report(y,y_pred))\n",
    "print(accuracy_score(y, y_pred))\n",
    "vot3 = accuracy_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNA + LIWC + SPLICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer  \n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O</td>\n",
       "      <td>http://www.youtube.com/watch?v=qsXHcwe3krw|||http://41.media.tumblr.com/tumblr_lfouy03PMA1qa1rooo1_500.jpg|||enfp and intj moments  https://www.youtube.com/watch?v=iz7lE1g4XM4  sportscenter not to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>'I'm finding the lack of me in these posts very alarming.|||Sex can be boring if it's in the same position often. For example me and my girlfriend are currently in an environment where we have to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/watch?v=fHiGbolFFGw|||Of course, to which I say I know; that's my blessing and my curse.|||Does being absolutely positive that you and your best friend c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the other day.  Esoteric gabbing about the nature of the universe and the idea that every rule and social code being arbitrary constructs created...|||Dear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>'You're fired.|||That's another silly misconception. That approaching is logically is going to be the key to unlocking whatever it is you think you are entitled to.   Nobody wants to be approached...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Type  \\\n",
       "0    O   \n",
       "1    A   \n",
       "2    O   \n",
       "3    O   \n",
       "4    A   \n",
       "\n",
       "                                                                                                                                                                                                     Posts  \n",
       "0  http://www.youtube.com/watch?v=qsXHcwe3krw|||http://41.media.tumblr.com/tumblr_lfouy03PMA1qa1rooo1_500.jpg|||enfp and intj moments  https://www.youtube.com/watch?v=iz7lE1g4XM4  sportscenter not to...  \n",
       "1  'I'm finding the lack of me in these posts very alarming.|||Sex can be boring if it's in the same position often. For example me and my girlfriend are currently in an environment where we have to ...  \n",
       "2  'Good one  _____   https://www.youtube.com/watch?v=fHiGbolFFGw|||Of course, to which I say I know; that's my blessing and my curse.|||Does being absolutely positive that you and your best friend c...  \n",
       "3  'Dear INTP,   I enjoyed our conversation the other day.  Esoteric gabbing about the nature of the universe and the idea that every rule and social code being arbitrary constructs created...|||Dear...  \n",
       "4  'You're fired.|||That's another silly misconception. That approaching is logically is going to be the key to unlocking whatever it is you think you are entitled to.   Nobody wants to be approached...  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data.csv',encoding='ISO-8859-1')\n",
    "df = df[['Type','Posts']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"[^a-zA-Z?.!,Â¿]+\", \" \", text) # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "\n",
    "    text = re.sub(r\"http\\S+\", \"\",text) #Removing URLs \n",
    "    #text = re.sub(r\"http\", \"\",text)\n",
    "    \n",
    "    html=re.compile(r'<.*?>') \n",
    "    \n",
    "    text = html.sub(r'',text) #Removing html tags\n",
    "    \n",
    "    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\" + '_'\n",
    "    for p in punctuations:\n",
    "        text = text.replace(p,'') #Removing punctuations\n",
    "        \n",
    "    text = [word.lower() for word in text.split() if word.lower() not in sw]\n",
    "    \n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    \n",
    "    text = \" \".join(text) #removing stopwords\n",
    "    \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text) #Removing emojis\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O    5563\n",
       "A    1774\n",
       "C     979\n",
       "N     217\n",
       "E     142\n",
       "Name: Type, dtype: int64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Posts</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O</td>\n",
       "      <td>http://www.youtube.com/watch?v=qsXHcwe3krw|||http://41.media.tumblr.com/tumblr_lfouy03PMA1qa1rooo1_500.jpg|||enfp and intj moments  https://www.youtube.com/watch?v=iz7lE1g4XM4  sportscenter not to...</td>\n",
       "      <td>http wwwyoutubecom watchv qsxhcwe krw http mediatumblrcom tumblr lfouy pma qa rooo jpg enfp intj moment wwwyoutubecom watchv iz le g xm sportscenter top ten play wwwyoutubecom watchv ucdfze etec p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>'I'm finding the lack of me in these posts very alarming.|||Sex can be boring if it's in the same position often. For example me and my girlfriend are currently in an environment where we have to ...</td>\n",
       "      <td>finding lack post alarming sex boring position often example girlfriend currently environment creatively use cowgirl missionary enough giving new meaning game theory hello entp grin take converse ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/watch?v=fHiGbolFFGw|||Of course, to which I say I know; that's my blessing and my curse.|||Does being absolutely positive that you and your best friend c...</td>\n",
       "      <td>good one wwwyoutubecom watchv fhigbolffgw course, say know blessing curse absolutely positive best friend could amazing couple count so, yes could madly love case reconciled feeling no, thank link...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the other day.  Esoteric gabbing about the nature of the universe and the idea that every rule and social code being arbitrary constructs created...|||Dear...</td>\n",
       "      <td>dear intp, enjoyed conversation day esoteric gabbing nature universe idea every rule social code arbitrary construct created dear entj sub, long time see sincerely, alpha none type hurt deep exist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>'You're fired.|||That's another silly misconception. That approaching is logically is going to be the key to unlocking whatever it is you think you are entitled to.   Nobody wants to be approached...</td>\n",
       "      <td>fired another silly misconception approaching logically going key unlocking whatever think entitled nobody want approached b guy really want go super duper long as vacation c mon guy bos listen ge...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Type  \\\n",
       "0    O   \n",
       "1    A   \n",
       "2    O   \n",
       "3    O   \n",
       "4    A   \n",
       "\n",
       "                                                                                                                                                                                                     Posts  \\\n",
       "0  http://www.youtube.com/watch?v=qsXHcwe3krw|||http://41.media.tumblr.com/tumblr_lfouy03PMA1qa1rooo1_500.jpg|||enfp and intj moments  https://www.youtube.com/watch?v=iz7lE1g4XM4  sportscenter not to...   \n",
       "1  'I'm finding the lack of me in these posts very alarming.|||Sex can be boring if it's in the same position often. For example me and my girlfriend are currently in an environment where we have to ...   \n",
       "2  'Good one  _____   https://www.youtube.com/watch?v=fHiGbolFFGw|||Of course, to which I say I know; that's my blessing and my curse.|||Does being absolutely positive that you and your best friend c...   \n",
       "3  'Dear INTP,   I enjoyed our conversation the other day.  Esoteric gabbing about the nature of the universe and the idea that every rule and social code being arbitrary constructs created...|||Dear...   \n",
       "4  'You're fired.|||That's another silly misconception. That approaching is logically is going to be the key to unlocking whatever it is you think you are entitled to.   Nobody wants to be approached...   \n",
       "\n",
       "                                                                                                                                                                                                      text  \n",
       "0  http wwwyoutubecom watchv qsxhcwe krw http mediatumblrcom tumblr lfouy pma qa rooo jpg enfp intj moment wwwyoutubecom watchv iz le g xm sportscenter top ten play wwwyoutubecom watchv ucdfze etec p...  \n",
       "1  finding lack post alarming sex boring position often example girlfriend currently environment creatively use cowgirl missionary enough giving new meaning game theory hello entp grin take converse ...  \n",
       "2  good one wwwyoutubecom watchv fhigbolffgw course, say know blessing curse absolutely positive best friend could amazing couple count so, yes could madly love case reconciled feeling no, thank link...  \n",
       "3  dear intp, enjoyed conversation day esoteric gabbing nature universe idea every rule social code arbitrary construct created dear entj sub, long time see sincerely, alpha none type hurt deep exist...  \n",
       "4  fired another silly misconception approaching logically going key unlocking whatever think entitled nobody want approached b guy really want go super duper long as vacation c mon guy bos listen ge...  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = df['Posts'].apply(lambda x: clean_text(x))\n",
    " \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "  \n",
    "# label_encoder object knows how to understand word labels.\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "  \n",
    "# Encode labels in column 'species'.\n",
    "df['news_category']= label_encoder.fit_transform(df['Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['http wwwyoutubecom watchv qsxhcwe krw http mediatumblrcom tumblr lfouy pma qa rooo jpg enfp intj moment wwwyoutubecom watchv iz le g xm sportscenter top ten play wwwyoutubecom watchv ucdfze etec prank life changing experience life http wwwyoutubecom watchv vxzeywwrdw http wwwyoutubecom watchv u ejam dp e repeat today may perc experience immerse last thing infj friend posted facebook committing suicide next day rest peace http vimeocom hello enfj sorry hear distress natural relationship perfection time every moment existence try figure hard time time growth, http wallpaperpassioncom upload friendship boy girl wallpaperjpg http assetsdornobcom wp content uploads round home designjpg welcome stuff http playeressencecom wp content uploads red red pokemon master jpg game set match prozac, wellbrutin, least thirty minute moving leg mean moving sitting desk chair , weed moderation maybe try edible healthier alternative basically come three item determined type whichever type want would likely use, given type cognitive function whatnot, left thing moderation sims indeed video game, good one note good one somewhat subjective completely promoting death given sim dear enfp favorite video game growing now, current favorite video game cool wwwyoutubecom watchv qypqt umzmy appears late sad someone everyone wait thought confidence good thing cherish time solitude b c revel within inner world whereas time workin enjoy time worry, people always around yo entp lady complimentary personality,well, hey main social outlet xbox live conversation even verbally fatigue quickly http wwwyoutubecom watchv gdhy rdfm really dig part http wwwyoutubecom watchv msqxffgh b banned thread requires get high backyard, roast eat marshmellows backyard conversing something intellectual, followed massage kiss http wwwyoutubecom watchv mw eou bmbe http wwwyoutubecom watchv v uyorhqok http wwwyoutubecom watchv slvmgfqq ti banned many b sentence could think b banned watching movie corner dunce banned health class clearly taught nothing peer pressure banned whole host reason http wwwyoutubecom watchv ircrv hgz two baby deer left right munching beetle middle using blood, two caveman diary today latest happening designated cave diary wall see pokemon world infj society everyone becomes optimist http wwwyoutubecom watchv zrceq jfefm http discovermagazinecom jul aug thing didnt know desert desertjpg http oysterignimgscom mediawiki apisigncom pokemon silver version dd dittogif http wwwserebiinet potw dp scizorjpg artist artist draw idea count forming something like signature welcome robot ranks, person downed self esteem cuz avid signature artist like proud banned taking room bed ya gotta learn share roach http wwwyoutubecom watchv w igimn aq banned much thundering, grumbling kind storm yep ahh old high school music heard age http wwwyoutubecom watchv dccrupcdb w failed public speaking class year ago sort learned could better position big part failure overloading like person mentality confirmed intj way http wwwyoutubecom watchv hgkli gec move denver area start new life',\n",
       "       'finding lack post alarming sex boring position often example girlfriend currently environment creatively use cowgirl missionary enough giving new meaning game theory hello entp grin take converse flirting acknowledge presence return word smooth wordplay cheeky grin lack balance hand eye coordination real iq test score internet iq test funny score higher now, like former response thread mention believe iq test banish know entp vanish site year half, return, find people still commenting post liking idea thought know entp http img imageshackus img f da b bbe jpg http imgadultdvdtalkcom c cab c think thing sometimes go old sherlock holmes quote perhaps, man special knowledge special power like own, rather encourages seek complex cheshirewolftumblrcom , post really never thought e j p real function judge use use ne ti dominates fe emotion rarely si also use ni due strength know though ingenious saying really want try see happens playing first person shooter back drive around want see look rock paper one best make lol guy lucky really high tumblr system hear new first person shooter game rocking hell soundtrack auto sound equipment shake heaven managed put couple p way connected thing ne ne dominates aware environment se dominates example shawn spencer patrick jane entps well charlie first admit get jealous like chalk w heart mixed dominate w like noticed like known upload clip mic away mouth hear anything ninja assassin style splatter tik tok really great song long mental block singer love beat make bounce dropio v swck mic really close mouth smokin ace assassin ball playing background sociable extrovert extrovert sociable sherlock movie entp normally played extj book estj said movie looked good except called sherlock holmes http photobucketcom album zz kamioo dirtywinchpng oh, never fear kissing guy kiss animal nothing vanish personal taste liking guy kissed know one sound pretty much like area going right trying figure way want take life want many thing biggest problem know operating impression female never looked boxy okay, help gay friend time one developed little crush get red described living worst nightmare trapped one place one one around dull wood serial killer would perfect place sadly tbh, biased, sound like shadowed infp think maybe hurt turned estj tell typical infp trait left check list sorry seems came bad time already reached quota infjs however, female like female make deal kick one antp leaning toward e easy entps intps identify also imagine entp interrogation would go little bit like jack except mechanical rigging shock treatment equipment abandoned building old car batty, jumper compliment trust psychopathic except emoticon weird one like laughing get hurt people running lawn mower http photobucketcom album zz kamioo thunderstormpnghttp photobucketcom album zz kamioo thunderstormbwpng http photobucketcom album zz kamioo cosmicstormpng like theme live know heart http wwwyoutubecom watchv j w havqbg usual leave thing end mean time time work thing work mine mbp pleasure meet damn, need trust instinct would closer going say infp exfp leaning toward way responded friends, even gay lesbian ones, always come advice bow entp master entps great entps able build building duck duck duck shotgun never hard sad losing someone like knew right give big pat back awesome always correct oh, tell stupid know play make laugh going take neuropsychology psychologist nightowl wake pm stay awake till personal opinion backed theory would suggest intps socially difficult intjs socially indifferent also use social situation need arises personal stock desktop downloaded random stock site stock photobuckets tell open photoshop glad like static thanks http photobucketcom album zz kamioo deathgrippng http photobucketcom album zz kamioo deathgripbwpng made friend several hour work constructed every line static http photobucketcom album zz kamioo statickittenpng get avatar later one fellow teammate psychologist keep around long enough diagnosis like toy diagnosis psychologist friend friend tell'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_corpora = df['text'].iloc[:2].values\n",
    "sample_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abandoned</th>\n",
       "      <th>able</th>\n",
       "      <th>ace</th>\n",
       "      <th>acknowledge</th>\n",
       "      <th>admit</th>\n",
       "      <th>advice</th>\n",
       "      <th>age</th>\n",
       "      <th>ago</th>\n",
       "      <th>ahh</th>\n",
       "      <th>alarming</th>\n",
       "      <th>...</th>\n",
       "      <th>wwwserebiinet</th>\n",
       "      <th>wwwyoutubecom</th>\n",
       "      <th>xbox</th>\n",
       "      <th>xm</th>\n",
       "      <th>ya</th>\n",
       "      <th>year</th>\n",
       "      <th>yep</th>\n",
       "      <th>yo</th>\n",
       "      <th>zrceq</th>\n",
       "      <th>zz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 661 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      abandoned  able  ace  acknowledge  admit  advice  age  ago  ahh  \\\n",
       "Doc0          0     0    0            0      0       0    1    1    1   \n",
       "Doc1          1     1    1            1      1       1    0    0    0   \n",
       "\n",
       "      alarming  ...  wwwserebiinet  wwwyoutubecom  xbox  xm  ya  year  yep  \\\n",
       "Doc0         0  ...              1             16     1   1   1     1    1   \n",
       "Doc1         1  ...              0              1     0   0   0     1    0   \n",
       "\n",
       "      yo  zrceq  zz  \n",
       "Doc0   1      1   0  \n",
       "Doc1   0      0   7  \n",
       "\n",
       "[2 rows x 661 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "wm = count_vectorizer.fit_transform(sample_corpora)\n",
    "\n",
    "doc_names = ['Doc{:d}'.format(idx) for idx, _ in enumerate(wm)]\n",
    "feat_names = count_vectorizer.get_feature_names()\n",
    "\n",
    "\n",
    "sample_df = pd.DataFrame(data=wm.toarray(), index=doc_names,columns=feat_names)\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test , y_train, y_test = train_test_split(df['text'].values,df['news_category'].values,test_size=0.2,random_state=123,stratify=df['news_category'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer() \n",
    "\n",
    "tfidf_train_vectors = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "tfidf_test_vectors = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train = tfidf_train_vectors.toarray()\n",
    "x_test = tfidf_test_vectors.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0  355]\n",
      " [   0    0    0    0  196]\n",
      " [   0    0    0    0   28]\n",
      " [   0    0    0    0   43]\n",
      " [   0    0    0    0 1113]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       355\n",
      "           1       0.00      0.00      0.00       196\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        43\n",
      "           4       0.64      1.00      0.78      1113\n",
      "\n",
      "    accuracy                           0.64      1735\n",
      "   macro avg       0.13      0.20      0.16      1735\n",
      "weighted avg       0.41      0.64      0.50      1735\n",
      "\n",
      "0.6414985590778098\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(gamma='auto')\n",
    "svm.fit(x_train,y_train)\n",
    "y_pred = svm.predict(x_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "svm2 = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[198  21   3   1 132]\n",
      " [ 17  83   2   1  93]\n",
      " [  3   3   0   0  22]\n",
      " [  9   5   0   2  27]\n",
      " [ 87  59  11   3 953]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.56      0.59       355\n",
      "           1       0.49      0.42      0.45       196\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.29      0.05      0.08        43\n",
      "           4       0.78      0.86      0.81      1113\n",
      "\n",
      "    accuracy                           0.71      1735\n",
      "   macro avg       0.44      0.38      0.39      1735\n",
      "weighted avg       0.69      0.71      0.70      1735\n",
      "\n",
      "0.7123919308357348\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0)\n",
    "gb.fit(x_train,y_train)\n",
    "y_pred = gb.predict(x_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "gb2 = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 178    5    0    0  172]\n",
      " [  14   56    0    0  126]\n",
      " [   2    0    0    0   26]\n",
      " [  10    2    0    1   30]\n",
      " [  32    5    0    0 1076]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.50      0.60       355\n",
      "           1       0.82      0.29      0.42       196\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       1.00      0.02      0.05        43\n",
      "           4       0.75      0.97      0.85      1113\n",
      "\n",
      "    accuracy                           0.76      1735\n",
      "   macro avg       0.67      0.36      0.38      1735\n",
      "weighted avg       0.75      0.76      0.72      1735\n",
      "\n",
      "0.7556195965417868\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_train,y_train)\n",
    "y_pred = lr.predict(x_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "lr2 = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 219    6    0    0  130]\n",
      " [  13   88    0    0   95]\n",
      " [   3    0    0    0   25]\n",
      " [  11    6    0    5   21]\n",
      " [  47   16    0    0 1050]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.62      0.68       355\n",
      "           1       0.76      0.45      0.56       196\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       1.00      0.12      0.21        43\n",
      "           4       0.79      0.94      0.86      1113\n",
      "\n",
      "    accuracy                           0.79      1735\n",
      "   macro avg       0.66      0.43      0.46      1735\n",
      "weighted avg       0.77      0.79      0.76      1735\n",
      "\n",
      "0.785014409221902\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(x_train,y_train)\n",
    "y_pred = xgb.predict(x_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "xgb2 = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[256  13   2   2  82]\n",
      " [ 35 105   1   4  51]\n",
      " [  8   1   2   0  17]\n",
      " [ 17   7   0   6  13]\n",
      " [168  76  15  18 836]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.72      0.61       355\n",
      "           1       0.52      0.54      0.53       196\n",
      "           2       0.10      0.07      0.08        28\n",
      "           3       0.20      0.14      0.16        43\n",
      "           4       0.84      0.75      0.79      1113\n",
      "\n",
      "    accuracy                           0.69      1735\n",
      "   macro avg       0.44      0.44      0.44      1735\n",
      "weighted avg       0.71      0.69      0.70      1735\n",
      "\n",
      "0.6945244956772334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "clf1 = XGBClassifier()\n",
    "clf3 = DecisionTreeClassifier()\n",
    "eclf1 = VotingClassifier(estimators=[('lr', clf1), ('dt', clf3)], voting='hard')\n",
    "eclf1.fit(x_train,y_train)\n",
    "y_pred = eclf1.predict(x_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "vot2 = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = [svm*100,gb*100,lr*100,xgb*100,vot*100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "score1 = [svm1*100,gb1*100,lr1*100,xgb1*100,vot1*100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "score2 = [svm2*100,gb2*100,lr2*100,xgb2*100,vot2*100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "score3 = [svm3*100,gb3*100,lr3*100,xgb3*100,vot3*100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#make variabel for save the result and to show it\n",
    "classifier = ('SVM','GradientBoosting','LR','XGB','VOting')\n",
    "y_pos = np.arange(len(classifier))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPLICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt2\n",
    "plt2.barh(y_pos, score, align='center', alpha=0.5,color='blue')\n",
    "plt2.yticks(y_pos, classifier)\n",
    "plt2.xlabel('Score')\n",
    "plt2.title('Classification Performance')\n",
    "plt2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbsAAAEWCAYAAAD/6zkuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAaaklEQVR4nO3debhddX3v8fcHAsUAZRAuEgVTh+IFhxjiQI2KijMVrLZgbSvWK9rHcmutWO6tD0ZaRWsrXgfsRYu0VTHUIgV72woCBcWKCaIMgoIyhiEIKJAWBL73j/WLbk9PkpNwcobfeb+eZz9nrd+avr+dnfM5v7XW3jtVhSRJPdtiuguQJGlzM+wkSd0z7CRJ3TPsJEndM+wkSd0z7CRJ3TPspHEkWZbk05tx/5cl2b9NJ8mnktyR5MIkz05y5WY45p5J7k6y5WTvezIl2SvJxUnuSvI/p7se9cGw05yV5DeTrGgBcFOSf06ydCqOXVX7VNW5bXYp8ELgUVX19Ko6v6r2eqjHSHJNkgNGjnldVW1XVQ881H2Pc6xKck97Lm9M8sGHEKrvAM6pqu2r6sOTWafmLsNOc1KStwEfAt4L7AbsCRwPHDQN5TwauKaq7pmGY0+mp1TVdsALgN8E3rgxGyeZ1yYfDVy2KQWM7EP6OYad5pwkOwDHAG+pqlOr6p6q+klVnVFVR65jm79PcnOSHyU5L8k+I8teluTydtrtxiRvb+27JPlikjuT3J7k/CRbtGXXJDkgyRuATwL7tVHRu5Psn+SGkf3vkeTUJKuT/DDJR1v7Y5Oc3dpuS/KZJDu2ZX/HEOBntP2+I8nCNgKb19ZZkOT0VttVSd44csxlSU5J8retX5clWTKR57eqrgDOB57Y9nVgOy15Z5ILkjx55DjXJPnjJN8G7klyNvA84KOt7l9OskOrY3WSa5O8c+R5PCzJV5Mcl+SHwLIkJyU5vo3U727LH5HkQ+1U8RVJnjpSw1FJrm79vDzJK0eWHZbkK0n+om37gyQvHVm+czsFvaotP21k2Tr7rWlQVT58zKkH8BLgfmDeetZZBnx6ZP53ge2BX2AYEV48suwm4NlteidgcZs+FvgrYKv2eDaQtuwa4IA2fRjwlZH97Q/c0Ka3BL4FHAdsC2wDLG3LHsdw+vMXgF2B84APjeznp8do8wuBWtvvtv7xbZ+LgNXA80f6/5/Ay1oNxwL/vp7nq4DHtem9gZuBNwBPBW4FntH287pW1y+M1HgxsAfwsNZ2LvA/Rvb9t8A/tud/IfBd4A0jz939wBHAPOBhwEnAbcC+rW9nAz8AfqfV8GcMp0nX7v/XgQUMf/wfAtwD7D6y/58wjFK3BH4PWDXy7/hPwHKGf/etgOe29vX228c0/L+f7gJ8+JjqB/Ba4OYNrLOMkbAbs2zH9st9hzZ/HfAm4BfHrHdM+yX9uHH28dMgYv1ht18LoXUG88h2BwPfHO8YbX5hq3teC5cHgO1Hlh8LnDTS/7NGlu0N/Md6jl3Aj4E7gKtboGwBfBz40zHrXjkSCtcAvztm+bm0sGtBcR+w98jyNwHnjjx3143Z/iTgEyPzRwDfGZl/EnDnevpyMXDQyP6vGlk2v/X1EcDuwIPATuPsY7399jH1D09jai76IbDLRK/vJNkyyfvaqa4fM/yCBtil/XwVwwjo2iT/lmS/1v4B4CrgS0m+n+SoTah1D+Daqrp/nLp2S/K5dur0x8CnR2rakAXA7VV110jbtcAjR+ZvHpleA2yzgedscVXtVFWPrap3VtWDDNff/qidyrszyZ2tTwtGtrt+PfvchWHEdO166hxv+1tGpv9jnPnt1s4k+Z2R0413Mpx+HX0ef/o8VNWaNrld68ftVXXHOMefSL81hQw7zUVfA+5lGAlNxG8y3LhyALADwwgJIABV9Y2qOgj4b8BpwCmt/a6q+qOqegzwCuBtSV6wkbVeD+y5jpB5L8Mo40lV9YvAb62tqVnfV5qsAnZOsv1I257AjRtZ34ZcD7ynqnYcecyvqpMnWOdtDKcRH72eOjf5q1uSPBr4BPD7wMOrakfgUn7+eVyX6xmewx3XsWxD/dYUMuw051TVj4CjgY8lOTjJ/CRbJXlpkj8fZ5PtGcLxhwynsd67dkGSrZO8NskOVfUThlN5D7ZlByZ5XJIAP2I4bfjgRpZ7IcM1wfcl2TbJNkmeNVLX3cCPkjwSGHtzzS3AY9bxHFwPXAAc2/b5ZIZrbJP93sJPAG9O8owMtk3y8jEhu041vE3iFOA9SbZv4fS2SaxzW4awXA2Q5PW0G2smUNtNwD8DxyfZqb2GntMWP6R+a/IZdpqTquovGX5pvpPhF931DH/dnzbO6n/LcOrsRuBy4N/HLP9t4Jp2KvHNDNcEAR4PnMUQSF8Djq+qczayzgeAX2W4GeU64AaGmygA3g0sZgjSfwJOHbP5scA722m0t4+z+9cwjFJXAV8A3lVVZ21MfROofwXDzR0fZbiedxXDdbCNcQTDTSPfB74CfBY4cZLquxz4S4Z/n1sYrud9dSN28dsMI88rGG5IeWvb72T0W5No7R1FkiR1y5GdJKl7hp0kqXuGnSSpe4adJKl7fmjqDLTLLrvUwoULp7sMSZpVVq5ceVtV7TreMsNuBlq4cCErVqyY7jIkaVZJcu26lnkaU5LUPcNOktQ9w06S1D3DTpLUPcNOktQ9w06S1D3DTpLUPcNOktQ931Q+A61aBcuWTXcVkjaW/29nLkd2kqTuGXaSpO4ZdpKk7hl2kqTuGXaSpO4ZdpKk7hl2kqTuGXaSpO4ZdpKk7hl2kqTuGXaSpO4ZdpKk7hl2kqTuzcmwS3JOkhePaXtrko8nWZrkwiRXtMfhI+scnGTvkfljkhwwlbVLkjbenAw74GTg0DFth7b2zwJvrqonAEuBNyV5eVvnYOCnYVdVR1fVWZu/XEnSQzFXw+7zwMuTbA2QZCGwAHghcFJVXQRQVbcB7wCOSvIrwCuADyS5OMljk5yU5NVtH9ckeXeSi5JckuQJrX3XJGcmuSzJJ5Ncm2SXKe+xJM1hczLsqup24ELgpa3pUOAUYB9g5ZjVVwD7VNUFwOnAkVW1qKquHmfXt1XVYuDjwNtb27uAs6tqH4aQ3XO8mpIcnmRFkhVr1qx+CL2TJI01J8OuGT2VufYU5kN1avu5EljYppcCnwOoqn8B7hhvw6o6oaqWVNWS+fN3nYRSJElrzeWw+0fgBUkWA/OraiVwObDvmPX2BS6b4D7vbT8fAOZNSpWSpIdszoZdVd0NnAOcyM9GdR8DDkuyCCDJw4H3A3/elt8FbL+Rh/oq8Bttfy8CdnpIhUuSNtqcDbvmZOAp7SdVdRPwW8AnklwBXACcWFVntPU/BxyZ5JtJHjvBY7wbeFGSS4FfB25mCE1J0hSZ06faquo0IGPazgOeto71v8rIWw+Aw0aWLRyZXgHs32Z/BLy4qu5Psh/wtKq6F0nSlJnTYTdF9gROSbIFcB/wxmmuR5LmHMNuM6uq7wFPne46JGkum+vX7CRJc4BhJ0nqnmEnSeqeYSdJ6p5hJ0nqnmEnSeqeYSdJ6p5hJ0nqnm8qn4EWLIBly6a7CknqhyM7SVL3DDtJUvcMO0lS9ww7SVL3DDtJUvcMO0lS9ww7SVL3fJ/dDLRqle+zk9S/qfw958hOktQ9w06S1D3DTpLUPcNOktQ9w06S1D3DTpLUPcNOktQ9w06S1D3DTpLUPcNOktQ9w06S1D3DTpLUPcNOktQ9w24CkuyR5AdJdm7zO7X5hUken+SLSa5OsjLJOUme09Y7LMnqJBcnuSzJ55PMn97eSNLcY9hNQFVdD3wceF9reh9wAnAz8E/ACVX12KraFzgCeMzI5suralFV7QPcBxwydZVLksDvs9sYxwErk7wVWAr8PvA7wNeq6vS1K1XVpcClYzdOMg/YFrhjSqqVJP2UYTdBVfWTJEcC/wK8qM3vA1y0gU0PSbIU2B34LnDGeCslORw4HGCHHfacvMIlSZ7G3EgvBW4CnjjewiRfSHJpklNHmpdX1SLgEcAlwJHjbVtVJ1TVkqpaMn/+rpNctiTNbYbdBCVZBLwQeCbwh0l2By4DFq9dp6peCRwG7Dx2+6oqhlHdc6agXEnSCMNuApKE4QaVt1bVdcAHgL8APgs8K8krRlZf392WS4GrN1uhkqRxec1uYt4IXFdVZ7b544HXA08HDgQ+mORDwC3AXcCfjWy79prdFsANDCM/SdIUMuwmoKpOYHirwdr5Bxg5fQm8bB3bnQSctDlrkyRtmKcxJUndM+wkSd0z7CRJ3TPsJEndM+wkSd0z7CRJ3TPsJEndM+wkSd0z7CRJ3TPsJEnd8+PCZqAFC2DZsumuQpL64chOktQ9w06S1D3DTpLUPcNOktQ9w06S1D3DTpLUPcNOktQ9w06S1D3fVD4DrVrlm8olzT2b8/eeIztJUvcMO0lS9ww7SVL3DDtJUvcMO0lS9ww7SVL3DDtJUvcMO0lS9ww7SVL3DDtJUvcMO0lS9ww7SVL3DLtJlOTucdqWJbkxycVJLk/ymumoTZLmMsNuahxXVYuAg4D/m2Sraa5HkuYUw24KVdX3gDXATtNdiyTNJYbdFEqyGPheVd06zrLDk6xIsmLNmtXTUJ0k9cuwmxp/mOQy4OvAe8ZboapOqKolVbVk/vxdp7Y6SeqcYTc1jquqfYBXAX+dZJvpLkiS5hLDbgpV1enACuB1012LJM0lht3kmp/khpHH28ZZ5xjgbUl87iVpisyb7gJ6UlUbDLCqWgnsNQXlSJIaRxeSpO4ZdpKk7hl2kqTuGXaSpO4ZdpKk7hl2kqTuGXaSpO4ZdpKk7hl2kqTuGXaSpO4ZdpKk7vnZmDPQggWwbNl0VyFJ/XBkJ0nqnmEnSeqeYSdJ6p5hJ0nqnmEnSeqeYSdJ6p5hJ0nqnu+zm4FWrfJ9dpL6MRN+nzmykyR1z7CTJHXPsJMkdc+wkyR1z7CTJHXPsJMkdc+wkyR1z7CTJHXPsJMkdc+wkyR1z7CTJHXPsJMkdW9CYZdktySfTfL9JCuTfC3JKzf1oEmWJXl7mz4myQGbuJ9FSV42Mn9YktVJLk5yWZLPJ5m/qXVO4HivSHLUZO1fkrR5bDDskgQ4DTivqh5TVfsChwKPGrPeJn2DQlUdXVVnbcq2wCLgZWPallfVoqraB7gPOGQT973B41XV6VX1vkncvyRpM5jIyO75wH1V9VdrG6rq2qr6SBtJnZ7kbODLSbZL8uUkFyW5JMlBa7dJ8idJvpvkK8BeI+0nJXl1m943yb+10eO/Jtm9tZ+b5P1JLmz7eHaSrYFjgEPaSO7nQq2F77bAHW1+YZKzk3y71bjnBtp/PcmlSb6V5Lzxjtf6/9GRfnw4yQVtBLy2T1skOT7JFUnOTPL/1i6TJE2NiYTdPsBF61m+GHh1VT0X+E/glVW1GHge8JcZrB0NLmIYGT1t7E6SbAV8pO1rX+BE4D0jq8yrqqcDbwXeVVX3AUfzs5Hc8rbeIUkuBm4EdgbOaO0fAf6mqp4MfAb48AbajwZeXFVPAV6xnuON2h1YChwIrB3x/RqwENgb+G1gv/GexCSHJ1mRZMWaNavHW0WStIk2+gaVJB9ro51vtKYzq+r2tYuB9yb5NnAW8EhgN+DZwBeqak1V/Rg4fZxd7wU8ETizhdU7+flTpae2nysZwmNdllfVIuARwCXAka19P+CzbfrvGEJpfe1fBU5K8kZgy/Ucb9RpVfVgVV3O0G/a/v6+td8MnDPehlV1QlUtqaol8+fvOsHDSZImYiJhdxnD6A2AqnoL8AJg7W/ke0bWfW1r37cFzi3ANhOsJcBlbdS0qKqeVFUvGll+b/v5ABP4hvWqKoZR3XMmePyx27+ZIXD3AFYmefgENrt3ZDqbclxJ0uSbSNidDWyT5PdG2tZ1h+MOwK1V9ZMkzwMe3drPAw5O8rAk2wO/Os62VwK7JtkPhtOaSfbZQG13AduvZ/lS4Oo2fQHDqVQYQvn89bUneWxVfb2qjgZWM4Teho43nq8Cr2rX7nYD9t/I7SVJD9GERkhJDgaOS/IOhl/89wB/DDxszOqfAc5IcgmwArii7eOiJMuBbwG3At8Ysx1VdV+7cePDSXZotX2IYWS5LucAR7XTnse2tkOSLGUI8huAw1r7EcCnkhzZ+vD6DbR/IMnjGUZoX261XzfO8TbkHxhGwpcD1zNc//zRBLeVJE2CDGf7tDkl2a6q7m6nQi8EntWu341rwYIldfjhK6auQEnajJYtm5rjJFlZVUvGW7ZJ743TRvtikh2BrYE/XV/QSZImn2E3Bapq/+muQZLmMj8bU5LUPcNOktQ9w06S1D3DTpLUPcNOktQ9w06S1D3DTpLUPcNOktQ9w06S1D0/QWUGWrBg6j5LTpLmAkd2kqTuGXaSpO4ZdpKk7hl2kqTuGXaSpO4ZdpKk7hl2kqTu+T67GWjVKt9nJ2nu2Zy/9xzZSZK6Z9hJkrpn2EmSumfYSZK6Z9hJkrpn2EmSumfYSZK6Z9hJkrpn2EmSumfYSZK6Z9hJkrpn2EmSumfYTVCSP0lyWZJvJ7k4ybuSHDtmnUVJvtOmr0ly/pjlFye5dCrrliQZdhOSZD/gQGBxVT0ZOAA4BzhkzKqHAiePzG+fZI+2j/8+FbVKkv4rw25idgduq6p7Aarqtqo6D7gjyTNG1vsNfj7sTuFngfiaMcskSVPEsJuYLwF7JPlukuOTPLe1n8wwmiPJM4Hbq+p7I9v9A/BrbfpXgTPWdYAkhydZkWTFmjWrJ78HkjSHGXYTUFV3A/sChwOrgeVJDgOWA69OsgX/9RQmwA8ZRn+HAt8B1qznGCdU1ZKqWjJ//q6boReSNHf5TeUTVFUPAOcC5ya5BHhdVZ2U5AfAc4FXAfuNs+ly4GPAYVNUqiRpDMNuApLsBTw4copyEXBtmz4ZOA74flXdMM7mX2C45vevwILNXKokaRyG3cRsB3wkyY7A/cBVDKc0Af4e+DBwxHgbVtVdwPsBkmz2QiVJ/5VhNwFVtRL4lXUsuw3Yapz2heO0XQM8cZLLkyRtgDeoSJK6Z9hJkrpn2EmSumfYSZK6Z9hJkrpn2EmSumfYSZK6Z9hJkrpn2EmSumfYSZK6Z9hJkrrnZ2POQAsWwLJl012FJPXDkZ0kqXuGnSSpe4adJKl7hp0kqXuGnSSpe4adJKl7hp0kqXuGnSSpe4adJKl7qarprkFjJLkLuHK665gkuwC3TXcRk8S+zDy99APsy2R4dFXtOt4CPy5sZrqyqpZMdxGTIckK+zLz9NKXXvoB9mVz8zSmJKl7hp0kqXuG3cx0wnQXMInsy8zUS1966QfYl83KG1QkSd1zZCdJ6p5hJ0nqnmE3wyR5SZIrk1yV5KjprmdjJDkxya1JLh1p2znJmUm+137uNJ01TkSSPZKck+TyJJcl+YPWPhv7sk2SC5N8q/Xl3a39l5J8vb3OlifZerprnYgkWyb5ZpIvtvnZ2o9rklyS5OIkK1rbrHt9ASTZMcnnk1yR5DtJ9puJfTHsZpAkWwIfA14K7A28Jsne01vVRjkJeMmYtqOAL1fV44Evt/mZ7n7gj6pqb+CZwFvav8Ns7Mu9wPOr6inAIuAlSZ4JvB84rqoeB9wBvGH6StwofwB8Z2R+tvYD4HlVtWjk/Wiz8fUF8H+Af6mqJwBPYfj3mXF9MexmlqcDV1XV96vqPuBzwEHTXNOEVdV5wO1jmg8C/qZN/w1w8FTWtCmq6qaquqhN38Xwn/eRzM6+VFXd3Wa3ao8Cng98vrXPir4keRTwcuCTbT7Mwn6sx6x7fSXZAXgO8NcAVXVfVd3JDOyLYTezPBK4fmT+htY2m+1WVTe16ZuB3aazmI2VZCHwVODrzNK+tFN/FwO3AmcCVwN3VtX9bZXZ8jr7EPAO4ME2/3BmZz9g+IPjS0lWJjm8tc3G19cvAauBT7XTy59Msi0zsC+GnaZMDe9zmTXvdUmyHfAPwFur6sejy2ZTX6rqgapaBDyK4ezBE6a3oo2X5EDg1qpaOd21TJKlVbWY4ZLFW5I8Z3ThLHp9zQMWAx+vqqcC9zDmlOVM6YthN7PcCOwxMv+o1jab3ZJkd4D289ZprmdCkmzFEHSfqapTW/Os7Mta7fTSOcB+wI5J1n427mx4nT0LeEWSaxhO7z+f4VrRbOsHAFV1Y/t5K/AFhj9CZuPr6wbghqr6epv/PEP4zbi+GHYzyzeAx7c7zLYGDgVOn+aaHqrTgde16dcB/ziNtUxIuxb018B3quqDI4tmY192TbJjm34Y8EKGa5DnAK9uq834vlTV/6qqR1XVQob/F2dX1WuZZf0ASLJtku3XTgMvAi5lFr6+qupm4Poke7WmFwCXMwP74ieozDBJXsZwbWJL4MSqes/0VjRxSU4G9mf4eo9bgHcBpwGnAHsC1wK/UVVjb2KZUZIsBc4HLuFn14f+N8N1u9nWlycz3CCwJcMft6dU1TFJHsMwQtoZ+CbwW1V17/RVOnFJ9gfeXlUHzsZ+tJq/0GbnAZ+tqvckeTiz7PUFkGQRw01DWwPfB15Pe60xg/pi2EmSuudpTElS9ww7SVL3DDtJUvcMO0lS9ww7SVL3DDtpjkvyJ+0bEb7dPoX/GdNdkzTZ5m14FUm9SrIfcCCwuKruTbILw/ulNnV/80Y+q1KaMRzZSXPb7sBta9+IXVW3VdWqJE9LckH7HrwLk2zfvhvvU+172L6Z5HkASQ5LcnqSs4Evt08IObFt980ks+abO9QvR3bS3PYl4Ogk3wXOApYDX2s/D6mqbyT5ReA/GL5LrqrqSUmewPCp/b/c9rMYeHJV3Z7kvQwf5/W77aPKLkxyVlXdM8V9k37KkZ00h7XvutsXOJzhq1qWA28Cbqqqb7R1ftxOTS4FPt3armD4GKi1YXfmyMdBvQg4qn2t0LnANgwfGyVNG0d20hxXVQ8whNK5SS4B3rIJuxkdtQV4VVVdOQnlSZPCkZ00hyXZK8njR5oWMXwrwu5JntbW2b59jc75wGtb2y8zjNbGC7R/BY5o3x5Bkqduvh5IE+PITprbtgM+0q6t3Q9cxXBK81Ot/WEM1+sOAI4HPt5Gf/cDh7U7OMfu808Zvrnj20m2AH7AcMenNG381gNJUvc8jSlJ6p5hJ0nqnmEnSeqeYSdJ6p5hJ0nqnmEnSeqeYSdJ6t7/B6E9uQKuFvMkAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt2\n",
    "plt2.barh(y_pos, score1, align='center', alpha=0.5,color='blue')\n",
    "plt2.yticks(y_pos, classifier)\n",
    "plt2.xlabel('Score')\n",
    "plt2.title('Classification Performance')\n",
    "plt2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt2\n",
    "plt2.barh(y_pos, score3, align='center', alpha=0.5,color='blue')\n",
    "plt2.yticks(y_pos, classifier)\n",
    "plt2.xlabel('Score')\n",
    "plt2.title('Classification Performance')\n",
    "plt2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPLICE + SNA + LIWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt2\n",
    "plt2.barh(y_pos, score2, align='center', alpha=0.5,color='blue')\n",
    "plt2.yticks(y_pos, classifier)\n",
    "plt2.xlabel('Score')\n",
    "plt2.title('Classification Performance')\n",
    "plt2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
